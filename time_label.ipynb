{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4bf1f43",
   "metadata": {},
   "source": [
    "# Time Label 处理速查（Dataset Interview 用）\n",
    "\n",
    "目标：把 raw time label 变成稳定、可复用、无泄漏的时间轴与特征；把关键检查项固化成可复制的代码块。  \n",
    "环境假设：Python 3.8 + numpy / pandas / scikit-learn（其余包若要装，现场再处理）。\n",
    "\n",
    "使用方式：打开数据后，从“0. 快速配置”往下跑；每个区块都可独立使用。  \n",
    "\n",
    "输出物：\n",
    "- `df`：完成时间解析与排序/去重后的主表\n",
    "- `df_feat`：包含时间衍生特征（可选包含 lag/rolling）\n",
    "- `train_df, test_df`：按时间切分后的训练/测试\n",
    "- `cv`：TimeSeriesSplit（或自定义切分）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b4ea8",
   "metadata": {},
   "source": [
    "## 0. 快速配置（只改这里）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce623a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ====== 必填：按数据改 ======\n",
    "TIME_COL = \"timestamp\"      # 时间列名\n",
    "ID_COL   = None             # 多实体（多股票/多设备）就填列名；单实体留 None\n",
    "TARGET_COL = \"y\"            # 目标列名（回归/分类标签）\n",
    "\n",
    "# 频率（需要对齐/重采样时用）：'1min','5min','1H','1D' 等；不对齐则 None\n",
    "FREQ = None\n",
    "\n",
    "# 时区策略：\n",
    "# - 若原始时间无时区但实际属于某个时区：ASSUME_TZ_IF_NAIVE 填那个时区（如 \"America/New_York\"）\n",
    "# - 输出统一为 OUTPUT_TZ（默认 UTC）\n",
    "ASSUME_TZ_IF_NAIVE = None\n",
    "OUTPUT_TZ = \"UTC\"\n",
    "\n",
    "# 去重策略：同一 (id,time) 多条时取 last/first；或按 agg 聚合\n",
    "DEDUP_STRATEGY = \"last\"     # \"last\" / \"first\" / \"agg\"\n",
    "\n",
    "# 对齐后缺测填充策略（仅在 FREQ 不为 None 时生效）\n",
    "FILL_NUMERIC = \"ffill\"      # \"ffill\" / \"bfill\" / \"interpolate\" / None\n",
    "MAX_GAP = None              # 允许 fill 的最大缺口（例如 '2H'）；不限制则 None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e126a423",
   "metadata": {},
   "source": [
    "## 1. 读入与时间解析（datetime + 时区 + NaT 统计）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9bae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time_column(df: pd.DataFrame,\n",
    "                      time_col: str,\n",
    "                      assume_tz_if_naive: str = None,\n",
    "                      output_tz: str = \"UTC\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1. 强制把时间列转成 pandas datetime\n",
    "    2. 若为 naive 且给了 assume_tz_if_naive：先 localize；否则保持 naive\n",
    "    3. 若带 tz 或已 localize：统一转 output_tz\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    out[time_col] = pd.to_datetime(out[time_col], errors=\"coerce\")\n",
    "\n",
    "    nat_rate = out[time_col].isna().mean()\n",
    "    if nat_rate > 0:\n",
    "        print(f\"[time parse] NaT rate: {nat_rate:.2%}\")\n",
    "\n",
    "    # 处理时区（pandas 时间列可能是 tz-aware 或 naive）\n",
    "    try:\n",
    "        tzinfo = out[time_col].dt.tz\n",
    "    except Exception:\n",
    "        tzinfo = None\n",
    "\n",
    "    if tzinfo is None:\n",
    "        if assume_tz_if_naive is not None:\n",
    "            out[time_col] = out[time_col].dt.tz_localize(\n",
    "                assume_tz_if_naive, ambiguous=\"infer\", nonexistent=\"shift_forward\"\n",
    "            )\n",
    "            tzinfo = out[time_col].dt.tz\n",
    "\n",
    "    if tzinfo is not None and output_tz is not None:\n",
    "        out[time_col] = out[time_col].dt.tz_convert(output_tz)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def basic_time_qc(df: pd.DataFrame, time_col: str, id_col: str = None):\n",
    "    \"\"\"基础 QC：NaT、范围、单调性、重复\"\"\"\n",
    "    d = df\n",
    "    if id_col is None:\n",
    "        print(\"rows:\", len(d))\n",
    "        print(\"NaT:\", int(d[time_col].isna().sum()))\n",
    "        if d[time_col].notna().any():\n",
    "            print(\"min/max:\", d[time_col].min(), d[time_col].max())\n",
    "        s = d.loc[d[time_col].notna(), time_col]\n",
    "        print(\"monotonic increasing:\", bool(s.is_monotonic_increasing))\n",
    "        print(\"duplicate timestamps:\", int(d.duplicated(subset=[time_col]).sum()))\n",
    "    else:\n",
    "        print(\"rows:\", len(d), \"unique ids:\", int(d[id_col].nunique(dropna=False)))\n",
    "        print(\"NaT:\", int(d[time_col].isna().sum()))\n",
    "        print(\"duplicate (id,time):\", int(d.duplicated(subset=[id_col, time_col]).sum()))\n",
    "\n",
    "\n",
    "# ====== 用法示例 ======\n",
    "# df_raw = pd.read_csv(\"xxx.csv\")\n",
    "# df = parse_time_column(df_raw, TIME_COL, ASSUME_TZ_IF_NAIVE, OUTPUT_TZ)\n",
    "# basic_time_qc(df, TIME_COL, ID_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2d90a",
   "metadata": {},
   "source": [
    "## 2. 排序 + 去重（同一 (id,time) 多条）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51374b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_and_dedup(df: pd.DataFrame,\n",
    "                   time_col: str,\n",
    "                   id_col: str = None,\n",
    "                   strategy: str = \"last\",\n",
    "                   agg: dict = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    strategy:\n",
    "      - 'last'/'first': 保留最后/最先记录\n",
    "      - 'agg': 按 (id,time) 聚合（agg dict 必须给）\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    key = [time_col] if id_col is None else [id_col, time_col]\n",
    "    out = out.sort_values(key)\n",
    "\n",
    "    if strategy in (\"last\", \"first\"):\n",
    "        keep = \"last\" if strategy == \"last\" else \"first\"\n",
    "        out = out.drop_duplicates(subset=key, keep=keep)\n",
    "    elif strategy == \"agg\":\n",
    "        if agg is None:\n",
    "            raise ValueError(\"strategy='agg' 需要提供 agg 字典，例如 {'price':'last','volume':'sum'}\")\n",
    "        out = out.groupby(key, as_index=False).agg(agg)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown strategy\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ====== 用法示例 ======\n",
    "# df = sort_and_dedup(df, TIME_COL, ID_COL, DEDUP_STRATEGY)\n",
    "# basic_time_qc(df, TIME_COL, ID_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0823d23",
   "metadata": {},
   "source": [
    "## 3. 对齐到规则频率（可选：生成完整时间栅格 + 缺测标记）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f932e35",
   "metadata": {},
   "source": [
    "对齐到规则频率的典型用途：分钟/小时/天级别建模；缺测需要显式处理（fill + missing mask）。  \n",
    "多实体场景：每个 id 单独对齐再拼接。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543e949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_to_frequency(df: pd.DataFrame,\n",
    "                       time_col: str,\n",
    "                       freq: str,\n",
    "                       id_col: str = None,\n",
    "                       fill_numeric: str = \"ffill\",\n",
    "                       max_gap: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    对齐：生成完整时间栅格，并把原始观测对齐上去。\n",
    "    - 数值列默认 ffill/bfill/interpolate\n",
    "    - 额外生成 _is_missing：对齐后该行是否为原始缺测\n",
    "    备注：max_gap 的严格实现较长，现场根据需要补。\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    if freq is None:\n",
    "        return out\n",
    "\n",
    "    numeric_cols = out.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if time_col in numeric_cols:\n",
    "        numeric_cols.remove(time_col)\n",
    "\n",
    "    def _fill(g):\n",
    "        g = g.set_index(time_col).sort_index()\n",
    "        full_idx = pd.date_range(g.index.min(), g.index.max(), freq=freq, tz=g.index.tz)\n",
    "        g2 = g.reindex(full_idx)\n",
    "        g2[\"_is_missing\"] = (g2[numeric_cols].isna().all(axis=1).astype(np.int8)\n",
    "                             if numeric_cols else 0)\n",
    "\n",
    "        if numeric_cols:\n",
    "            if fill_numeric == \"ffill\":\n",
    "                g2[numeric_cols] = g2[numeric_cols].ffill()\n",
    "            elif fill_numeric == \"bfill\":\n",
    "                g2[numeric_cols] = g2[numeric_cols].bfill()\n",
    "            elif fill_numeric == \"interpolate\":\n",
    "                g2[numeric_cols] = g2[numeric_cols].interpolate(limit_direction=\"both\")\n",
    "            elif fill_numeric is None:\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"unknown fill_numeric\")\n",
    "\n",
    "        g2.index.name = time_col\n",
    "        return g2.reset_index()\n",
    "\n",
    "    if id_col is None:\n",
    "        return _fill(out)\n",
    "\n",
    "    pieces = []\n",
    "    for _id, g in out.groupby(id_col, sort=False):\n",
    "        g2 = _fill(g)\n",
    "        g2[id_col] = _id\n",
    "        pieces.append(g2)\n",
    "    out2 = pd.concat(pieces, ignore_index=True).sort_values([id_col, time_col])\n",
    "    return out2\n",
    "\n",
    "\n",
    "# ====== 用法示例 ======\n",
    "# if FREQ is not None:\n",
    "#     df = align_to_frequency(df, TIME_COL, FREQ, ID_COL, FILL_NUMERIC, MAX_GAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82078f",
   "metadata": {},
   "source": [
    "## 4. 时间衍生特征：日历特征 + 周期 sin/cos + 相对时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a00822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_calendar_features(df: pd.DataFrame, time_col: str) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    t = out[time_col]\n",
    "\n",
    "    out[\"year\"] = t.dt.year\n",
    "    out[\"month\"] = t.dt.month\n",
    "    out[\"day\"] = t.dt.day\n",
    "    out[\"dayofweek\"] = t.dt.dayofweek\n",
    "    out[\"hour\"] = t.dt.hour\n",
    "    out[\"minute\"] = t.dt.minute\n",
    "\n",
    "    out[\"is_weekend\"] = (out[\"dayofweek\"] >= 5).astype(np.int8)\n",
    "    out[\"is_month_start\"] = t.dt.is_month_start.astype(np.int8)\n",
    "    out[\"is_month_end\"] = t.dt.is_month_end.astype(np.int8)\n",
    "    out[\"is_quarter_start\"] = t.dt.is_quarter_start.astype(np.int8)\n",
    "    out[\"is_quarter_end\"] = t.dt.is_quarter_end.astype(np.int8)\n",
    "\n",
    "    try:\n",
    "        out[\"weekofyear\"] = t.dt.isocalendar().week.astype(int)\n",
    "    except Exception:\n",
    "        out[\"weekofyear\"] = t.dt.weekofyear\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_cyclical_features(df: pd.DataFrame, col: str, period: int, prefix: str = None) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    x = out[col].astype(float)\n",
    "    p = float(period)\n",
    "    name = prefix or col\n",
    "    out[f\"{name}_sin\"] = np.sin(2*np.pi*x/p)\n",
    "    out[f\"{name}_cos\"] = np.cos(2*np.pi*x/p)\n",
    "    return out\n",
    "\n",
    "\n",
    "def add_relative_time_features(df: pd.DataFrame, time_col: str, id_col: str = None) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    def _rel(g):\n",
    "        g = g.sort_values(time_col).copy()\n",
    "        dt_prev = g[time_col].diff()\n",
    "        g[\"dt_from_prev_sec\"] = dt_prev.dt.total_seconds().fillna(0.0)\n",
    "        dt_start = g[time_col] - g[time_col].iloc[0]\n",
    "        g[\"dt_from_start_sec\"] = dt_start.dt.total_seconds()\n",
    "        return g\n",
    "\n",
    "    if id_col is None:\n",
    "        return _rel(out)\n",
    "    return out.groupby(id_col, group_keys=False, sort=False).apply(_rel)\n",
    "\n",
    "\n",
    "# ====== 用法示例 ======\n",
    "# df_feat = add_calendar_features(df, TIME_COL)\n",
    "# df_feat = add_cyclical_features(df_feat, \"hour\", 24)\n",
    "# df_feat = add_cyclical_features(df_feat, \"dayofweek\", 7)\n",
    "# df_feat = add_cyclical_features(df_feat, \"month\", 12)\n",
    "# df_feat = add_relative_time_features(df_feat, TIME_COL, ID_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d25f4",
   "metadata": {},
   "source": [
    "## 5. Lag / Rolling 特征（防泄漏：先 shift 再 rolling）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32a7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_features(df: pd.DataFrame,\n",
    "                    cols,\n",
    "                    lags=(1,2,3,5,10),\n",
    "                    time_col: str = None,\n",
    "                    id_col: str = None) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if isinstance(cols, str):\n",
    "        cols = [cols]\n",
    "\n",
    "    def _lag(g):\n",
    "        g = g.sort_values(time_col).copy() if time_col is not None else g.copy()\n",
    "        for c in cols:\n",
    "            for L in lags:\n",
    "                g[f\"{c}_lag{L}\"] = g[c].shift(L)\n",
    "        return g\n",
    "\n",
    "    if id_col is None:\n",
    "        return _lag(out)\n",
    "    return out.groupby(id_col, group_keys=False, sort=False).apply(_lag)\n",
    "\n",
    "\n",
    "def add_rolling_features(df: pd.DataFrame,\n",
    "                         cols,\n",
    "                         windows=(5,10,20),\n",
    "                         stats=(\"mean\",\"std\",\"min\",\"max\"),\n",
    "                         time_col: str = None,\n",
    "                         id_col: str = None,\n",
    "                         min_periods: int = 1,\n",
    "                         shift_before: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"rolling 特征：默认 shift_before=1，确保 rolling 只用历史信息\"\"\"\n",
    "    out = df.copy()\n",
    "    if isinstance(cols, str):\n",
    "        cols = [cols]\n",
    "\n",
    "    def _roll(g):\n",
    "        g = g.sort_values(time_col).copy() if time_col is not None else g.copy()\n",
    "        for c in cols:\n",
    "            base = g[c].shift(shift_before)\n",
    "            for w in windows:\n",
    "                r = base.rolling(window=w, min_periods=min_periods)\n",
    "                if \"mean\" in stats: g[f\"{c}_rmean{w}\"] = r.mean()\n",
    "                if \"std\"  in stats: g[f\"{c}_rstd{w}\"]  = r.std(ddof=0)\n",
    "                if \"min\"  in stats: g[f\"{c}_rmin{w}\"]  = r.min()\n",
    "                if \"max\"  in stats: g[f\"{c}_rmax{w}\"]  = r.max()\n",
    "        return g\n",
    "\n",
    "    if id_col is None:\n",
    "        return _roll(out)\n",
    "    return out.groupby(id_col, group_keys=False, sort=False).apply(_roll)\n",
    "\n",
    "\n",
    "# ====== 用法示例 ======\n",
    "# FEATURE_BASE_COLS = [\"x1\",\"x2\"]   # 数值特征列\n",
    "# df_feat = add_lag_features(df_feat, FEATURE_BASE_COLS, lags=(1,2,3), time_col=TIME_COL, id_col=ID_COL)\n",
    "# df_feat = add_rolling_features(df_feat, FEATURE_BASE_COLS, windows=(5,10,20), time_col=TIME_COL, id_col=ID_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e84454",
   "metadata": {},
   "source": [
    "## 6. 时间切分：Train/Test + TimeSeriesSplit（walk-forward）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac8e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def time_based_train_test_split(df: pd.DataFrame,\n",
    "                                time_col: str,\n",
    "                                test_size: float = 0.2,\n",
    "                                id_col: str = None):\n",
    "    \"\"\"按时间分位数切分：最后一段时间做 test\"\"\"\n",
    "    out = df.sort_values([time_col] if id_col is None else [time_col, id_col]).copy()\n",
    "    out = out[out[time_col].notna()].copy()\n",
    "\n",
    "    split_time = out[time_col].quantile(1 - test_size)\n",
    "    train = out[out[time_col] <= split_time].copy()\n",
    "    test  = out[out[time_col] >  split_time].copy()\n",
    "\n",
    "    print(\"split_time:\", split_time)\n",
    "    print(\"train rows:\", len(train), \"test rows:\", len(test))\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def build_time_series_cv(n_splits: int = 5):\n",
    "    return TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "\n",
    "# ====== 用法示例 ======\n",
    "# train_df, test_df = time_based_train_test_split(df_feat, TIME_COL, test_size=0.2, id_col=ID_COL)\n",
    "# cv = build_time_series_cv(n_splits=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b00b77",
   "metadata": {},
   "source": [
    "## 7. sklearn 建模骨架（预处理 + baseline 模型 + 指标）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f2c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "def make_preprocess(numeric_features, categorical_features):\n",
    "    numeric_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    ])\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ])\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_pipe, numeric_features),\n",
    "            (\"cat\", cat_pipe, categorical_features),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.3,\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "\n",
    "def fit_evaluate_regression(train_df, test_df, target_col, time_col, drop_cols=()):\n",
    "    df_tr = train_df.copy()\n",
    "    df_te = test_df.copy()\n",
    "\n",
    "    drop = set([target_col, time_col]) | set(drop_cols)\n",
    "    if ID_COL is not None:\n",
    "        drop.add(ID_COL)\n",
    "\n",
    "    X_tr = df_tr.drop(columns=[c for c in drop if c in df_tr.columns])\n",
    "    y_tr = df_tr[target_col].values\n",
    "    X_te = df_te.drop(columns=[c for c in drop if c in df_te.columns])\n",
    "    y_te = df_te[target_col].values\n",
    "\n",
    "    numeric_features = X_tr.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = [c for c in X_tr.columns if c not in numeric_features]\n",
    "\n",
    "    pre = make_preprocess(numeric_features, categorical_features)\n",
    "\n",
    "    models = {\n",
    "        \"ridge\": Ridge(alpha=1.0, random_state=0),\n",
    "        \"rf\": RandomForestRegressor(n_estimators=300, random_state=0, n_jobs=-1),\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        pred = pipe.predict(X_te)\n",
    "        rmse = mean_squared_error(y_te, pred, squared=False)\n",
    "        print(f\"[{name}] RMSE:\", rmse)\n",
    "\n",
    "\n",
    "def fit_evaluate_classification(train_df, test_df, target_col, time_col, drop_cols=()):\n",
    "    df_tr = train_df.copy()\n",
    "    df_te = test_df.copy()\n",
    "\n",
    "    drop = set([target_col, time_col]) | set(drop_cols)\n",
    "    if ID_COL is not None:\n",
    "        drop.add(ID_COL)\n",
    "\n",
    "    X_tr = df_tr.drop(columns=[c for c in drop if c in df_tr.columns])\n",
    "    y_tr = df_tr[target_col].values\n",
    "    X_te = df_te.drop(columns=[c for c in drop if c in df_te.columns])\n",
    "    y_te = df_te[target_col].values\n",
    "\n",
    "    numeric_features = X_tr.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = [c for c in X_tr.columns if c not in numeric_features]\n",
    "\n",
    "    pre = make_preprocess(numeric_features, categorical_features)\n",
    "\n",
    "    models = {\n",
    "        \"logreg\": LogisticRegression(max_iter=2000),\n",
    "        \"rf\": RandomForestClassifier(n_estimators=400, random_state=0, n_jobs=-1),\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        proba = pipe.predict_proba(X_te)[:, 1] if hasattr(pipe[-1], \"predict_proba\") else pipe.decision_function(X_te)\n",
    "        auc = roc_auc_score(y_te, proba)\n",
    "        print(f\"[{name}] AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe1b569",
   "metadata": {},
   "source": [
    "## 8. 一键流水线（time parse → dedup → align → calendar/cycle/relative → split → baseline）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb245257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_time_label_feature_table(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = parse_time_column(df_raw, TIME_COL, ASSUME_TZ_IF_NAIVE, OUTPUT_TZ)\n",
    "    df = sort_and_dedup(df, TIME_COL, ID_COL, DEDUP_STRATEGY)\n",
    "\n",
    "    if FREQ is not None:\n",
    "        df = align_to_frequency(df, TIME_COL, FREQ, ID_COL, FILL_NUMERIC, MAX_GAP)\n",
    "\n",
    "    df_feat = df.copy()\n",
    "    df_feat = add_calendar_features(df_feat, TIME_COL)\n",
    "\n",
    "    # 周期编码（按需要启用）\n",
    "    if \"hour\" in df_feat.columns:\n",
    "        df_feat = add_cyclical_features(df_feat, \"hour\", 24)\n",
    "    if \"dayofweek\" in df_feat.columns:\n",
    "        df_feat = add_cyclical_features(df_feat, \"dayofweek\", 7)\n",
    "    if \"month\" in df_feat.columns:\n",
    "        df_feat = add_cyclical_features(df_feat, \"month\", 12)\n",
    "\n",
    "    df_feat = add_relative_time_features(df_feat, TIME_COL, ID_COL)\n",
    "    return df_feat\n",
    "\n",
    "\n",
    "# ====== 端到端示例（现场按数据路径与列名改） ======\n",
    "# df_raw = pd.read_parquet(\"data.parquet\")  # or pd.read_csv(\"data.csv\")\n",
    "# df_feat = build_time_label_feature_table(df_raw)\n",
    "# basic_time_qc(df_feat, TIME_COL, ID_COL)\n",
    "# train_df, test_df = time_based_train_test_split(df_feat, TIME_COL, test_size=0.2, id_col=ID_COL)\n",
    "# fit_evaluate_regression(train_df, test_df, TARGET_COL, TIME_COL)        # 回归\n",
    "# fit_evaluate_classification(train_df, test_df, TARGET_COL, TIME_COL)    # 二分类\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae167a29",
   "metadata": {},
   "source": [
    "## 9. 最小化检查清单（最后 2 分钟扫一遍）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e453601d",
   "metadata": {},
   "source": [
    "- 时间列已转 datetime；NaT 比例已打印  \n",
    "- (id,time) 排序且去重策略明确（last/first/agg）  \n",
    "- train/test 严格按时间切分；无随机打乱  \n",
    "- lag/rolling 仅使用历史信息（shift 后再 rolling）  \n",
    "- 对齐后缺测有 `_is_missing` 或类似标记（若做了对齐）  \n",
    "- baseline 模型已跑通并有指标（RMSE/AUC）  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b580e07",
   "metadata": {},
   "source": [
    "## 10. 金融数据中的不均匀时间（Irregular / Event-driven Time）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8988d3",
   "metadata": {},
   "source": [
    "金融 tick / trade 数据是事件驱动的，不满足均匀时间假设。  \n",
    "处理原则：**不强行插值**，要么在事件时间中建模，要么通过 bar 机制把不均匀性显式吸收。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc01c1e1",
   "metadata": {},
   "source": [
    "### 10.1 Event-time 建模（不对齐、不插值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_event_time_features(df: pd.DataFrame,\n",
    "                             time_col: str,\n",
    "                             id_col: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Event-time 特征：\n",
    "    - event_idx: 事件序号\n",
    "    - dt_from_prev_sec: 距上一个事件的时间间隔\n",
    "    - log_dt_from_prev_sec\n",
    "    适用于 tick / trade 级别数据\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    def _evt(g):\n",
    "        g = g.sort_values(time_col).copy()\n",
    "        g[\"event_idx\"] = np.arange(len(g), dtype=np.int64)\n",
    "        dt = g[time_col].diff().dt.total_seconds()\n",
    "        g[\"dt_from_prev_sec\"] = dt.fillna(0.0)\n",
    "        g[\"log_dt_from_prev_sec\"] = np.log1p(g[\"dt_from_prev_sec\"])\n",
    "        return g\n",
    "\n",
    "    if id_col is None:\n",
    "        return _evt(out)\n",
    "    return out.groupby(id_col, group_keys=False, sort=False).apply(_evt)\n",
    "\n",
    "\n",
    "# ====== 用法示例 ======\n",
    "# df_evt = add_event_time_features(df, TIME_COL, ID_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f416f",
   "metadata": {},
   "source": [
    "### 10.2 Time bars（固定时间桶）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d34d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_time_bars(df: pd.DataFrame,\n",
    "                   time_col: str,\n",
    "                   freq: str,\n",
    "                   price_col: str,\n",
    "                   volume_col: str = None,\n",
    "                   id_col: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    固定时间 bar：\n",
    "    - price: last\n",
    "    - return: log return\n",
    "    - volume: sum\n",
    "    - n_trades: count\n",
    "    - has_trade: 是否有交易\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    def _bar(g):\n",
    "        g = g.set_index(time_col).sort_index()\n",
    "        res = {}\n",
    "        res[\"price\"] = g[price_col].resample(freq).last()\n",
    "        res[\"n_trades\"] = g[price_col].resample(freq).count()\n",
    "        if volume_col is not None:\n",
    "            res[\"volume\"] = g[volume_col].resample(freq).sum()\n",
    "        bar = pd.concat(res, axis=1)\n",
    "        bar[\"has_trade\"] = (bar[\"n_trades\"] > 0).astype(np.int8)\n",
    "        bar[\"return\"] = np.log(bar[\"price\"] / bar[\"price\"].shift(1))\n",
    "        bar = bar.reset_index()\n",
    "        return bar\n",
    "\n",
    "    if id_col is None:\n",
    "        return _bar(out)\n",
    "\n",
    "    pieces = []\n",
    "    for _id, g in out.groupby(id_col, sort=False):\n",
    "        b = _bar(g)\n",
    "        b[id_col] = _id\n",
    "        pieces.append(b)\n",
    "\n",
    "    return pd.concat(pieces, ignore_index=True).sort_values([id_col, time_col])\n",
    "\n",
    "\n",
    "# ====== 用法示例 ======\n",
    "# bar_df = make_time_bars(df, TIME_COL, \"1min\", price_col=\"price\", volume_col=\"volume\", id_col=ID_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d843f68",
   "metadata": {},
   "source": [
    "### 10.3 Volume / Dollar bars（市场活动时间）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e364be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_volume_bars(df: pd.DataFrame,\n",
    "                     time_col: str,\n",
    "                     price_col: str,\n",
    "                     volume_col: str,\n",
    "                     volume_threshold: float,\n",
    "                     id_col: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Volume bars：累计成交量达到阈值生成一个 bar\n",
    "    输出包含 bar 的时间跨度（duration）\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    def _vb(g):\n",
    "        g = g.sort_values(time_col).copy()\n",
    "        bars = []\n",
    "        cum_vol = 0.0\n",
    "        start_idx = 0\n",
    "\n",
    "        for i, row in g.iterrows():\n",
    "            cum_vol += row[volume_col]\n",
    "            if cum_vol >= volume_threshold:\n",
    "                chunk = g.iloc[start_idx:i+1]\n",
    "                bar = {\n",
    "                    time_col: chunk[time_col].iloc[-1],\n",
    "                    \"price\": chunk[price_col].iloc[-1],\n",
    "                    \"volume\": chunk[volume_col].sum(),\n",
    "                    \"n_trades\": len(chunk),\n",
    "                    \"duration_sec\": (\n",
    "                        chunk[time_col].iloc[-1] - chunk[time_col].iloc[0]\n",
    "                    ).total_seconds(),\n",
    "                }\n",
    "                bars.append(bar)\n",
    "                cum_vol = 0.0\n",
    "                start_idx = i + 1\n",
    "\n",
    "        return pd.DataFrame(bars)\n",
    "\n",
    "    if id_col is None:\n",
    "        return _vb(out)\n",
    "\n",
    "    pieces = []\n",
    "    for _id, g in out.groupby(id_col, sort=False):\n",
    "        b = _vb(g)\n",
    "        b[id_col] = _id\n",
    "        pieces.append(b)\n",
    "\n",
    "    return pd.concat(pieces, ignore_index=True).sort_values([id_col, time_col])\n",
    "\n",
    "\n",
    "# ====== 用法示例 ======\n",
    "# vbar_df = make_volume_bars(df, TIME_COL, \"price\", \"volume\", volume_threshold=1e6, id_col=ID_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efce89e",
   "metadata": {},
   "source": [
    "### 10.4 什么时候用哪种（速查）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7a17df",
   "metadata": {},
   "source": [
    "- **Tick / HFT / microstructure**：event-time + Δt 特征  \n",
    "- **常规 alpha / 中低频**：time bars（1–5min）  \n",
    "- **成交活跃度差异大**：volume / dollar bars  \n",
    "- **研究流动性 / 信息到达**：duration（Δt）本身就是信号  \n",
    "\n",
    "关键点：不均匀性不是噪声，而是金融数据的重要组成部分。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5fe214",
   "metadata": {},
   "source": [
    "## 10. 金融 tick 不均匀：Event-time / Time bars / Volume & Dollar bars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f49b32",
   "metadata": {},
   "source": [
    "金融 tick/trade/quote 是事件驱动，不是时钟驱动。处理路径分三类：\n",
    "\n",
    "- **Event-time（事件时间）**：不对齐、不插值；把 `Δt` 当成特征\n",
    "- **Time bars（固定时间桶）**：按固定频率聚合；显式记录空桶/交易次数\n",
    "- **Volume / Dollar bars（活动时间）**：按成交量/成交额切桶；每个 bar 的 duration 自带不均匀信息\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a49e2a",
   "metadata": {},
   "source": [
    "### 10.1 Event-time：保留不规则时间戳 + 显式 gap 特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_event_time_features(df: pd.DataFrame,\n",
    "                           time_col: str,\n",
    "                           id_col: str = None,\n",
    "                           add_log_gap: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    事件时间特征：不做 resample。每条记录是一个 event。\n",
    "    生成：\n",
    "      - event_idx：事件序号（每个 id 内部）\n",
    "      - gap_sec：与上一个 event 的时间差（秒）\n",
    "      - log_gap_sec：log(1+gap)\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    def _one(g):\n",
    "        g = g.sort_values(time_col).copy()\n",
    "        g[\"event_idx\"] = np.arange(len(g), dtype=np.int64)\n",
    "        gap = g[time_col].diff()\n",
    "        g[\"gap_sec\"] = gap.dt.total_seconds().fillna(0.0)\n",
    "        if add_log_gap:\n",
    "            g[\"log_gap_sec\"] = np.log1p(g[\"gap_sec\"].clip(lower=0.0))\n",
    "        return g\n",
    "\n",
    "    if id_col is None:\n",
    "        return _one(out)\n",
    "    return out.groupby(id_col, group_keys=False, sort=False).apply(_one)\n",
    "\n",
    "\n",
    "# ====== 用法示例 ======\n",
    "# df_evt = add_event_time_features(df, TIME_COL, ID_COL)\n",
    "# df_evt = add_lag_features(df_evt, cols=[\"price\",\"spread\"], lags=(1,2,3), time_col=TIME_COL, id_col=ID_COL)\n",
    "# df_evt = add_rolling_features(df_evt, cols=[\"return\",\"gap_sec\"], windows=(20,50,100), time_col=TIME_COL, id_col=ID_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d880cc9",
   "metadata": {},
   "source": [
    "### 10.2 Time bars：固定时间桶聚合（OHLC/VWAP/成交量/交易次数/空桶标记）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_time_bars(df: pd.DataFrame,\n",
    "                   time_col: str,\n",
    "                   freq: str,\n",
    "                   price_col: str = None,\n",
    "                   volume_col: str = None,\n",
    "                   id_col: str = None,\n",
    "                   tz: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    固定时间桶（time bars）。\n",
    "    输入 df 可为 tick（trade）或 quote 更新。\n",
    "    - price_col 给了则生成 ohlc/last\n",
    "    - volume_col 给了则生成 volume_sum\n",
    "    - 生成 n_events（桶内事件数）与 has_event（是否为空桶）\n",
    "    \"\"\"\n",
    "    out = df.copy().sort_values([time_col] if id_col is None else [id_col, time_col])\n",
    "\n",
    "    def _bars(g):\n",
    "        g = g.set_index(time_col).sort_index()\n",
    "        if tz is not None:\n",
    "            # 若索引无 tz，则 localize；若有 tz，则 convert\n",
    "            if g.index.tz is None:\n",
    "                g.index = g.index.tz_localize(tz, ambiguous=\"infer\", nonexistent=\"shift_forward\")\n",
    "            else:\n",
    "                g.index = g.index.tz_convert(tz)\n",
    "\n",
    "        agg_parts = {}\n",
    "\n",
    "        # 事件数：桶内 count\n",
    "        agg_parts[\"n_events\"] = g.iloc[:, 0].resample(freq).size().rename(\"n_events\") if len(g.columns) else g.resample(freq).size().rename(\"n_events\")\n",
    "\n",
    "        if price_col is not None and price_col in g.columns:\n",
    "            o = g[price_col].resample(freq).first().rename(\"open\")\n",
    "            h = g[price_col].resample(freq).max().rename(\"high\")\n",
    "            l = g[price_col].resample(freq).min().rename(\"low\")\n",
    "            c = g[price_col].resample(freq).last().rename(\"close\")\n",
    "            last = c.rename(\"last\")\n",
    "            agg_parts.update({\"open\": o, \"high\": h, \"low\": l, \"close\": c, \"last\": last})\n",
    "\n",
    "            # return（基于 last）\n",
    "            agg_parts[\"ret\"] = np.log(last).diff().rename(\"ret\")\n",
    "\n",
    "        if volume_col is not None and volume_col in g.columns:\n",
    "            vol = g[volume_col].resample(freq).sum(min_count=1).rename(\"volume_sum\")\n",
    "            agg_parts[\"volume_sum\"] = vol\n",
    "\n",
    "            # vwap（若 price_col+volume_col 都存在）\n",
    "            if price_col is not None and price_col in g.columns:\n",
    "                vwap = (g[price_col] * g[volume_col]).resample(freq).sum(min_count=1) / vol\n",
    "                agg_parts[\"vwap\"] = vwap.rename(\"vwap\")\n",
    "\n",
    "        res = pd.concat(agg_parts.values(), axis=1)\n",
    "        res[\"has_event\"] = (res[\"n_events\"].fillna(0) > 0).astype(np.int8)\n",
    "        res.index.name = time_col\n",
    "        return res.reset_index()\n",
    "\n",
    "    if id_col is None:\n",
    "        return _bars(out)\n",
    "\n",
    "    pieces = []\n",
    "    for _id, g in out.groupby(id_col, sort=False):\n",
    "        b = _bars(g)\n",
    "        b[id_col] = _id\n",
    "        pieces.append(b)\n",
    "    return pd.concat(pieces, ignore_index=True).sort_values([id_col, time_col])\n",
    "\n",
    "\n",
    "# ====== 用法示例 ======\n",
    "# bars = make_time_bars(df, TIME_COL, freq=\"1min\", price_col=\"price\", volume_col=\"size\", id_col=ID_COL)\n",
    "# bars = add_calendar_features(bars, TIME_COL)\n",
    "# train_df, test_df = time_based_train_test_split(bars, TIME_COL, test_size=0.2, id_col=ID_COL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834648a",
   "metadata": {},
   "source": [
    "### 10.3 Volume bars / Dollar bars：按市场活动切桶（每个 bar 自带 duration）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ffaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_volume_bars(df: pd.DataFrame,\n",
    "                     time_col: str,\n",
    "                     price_col: str,\n",
    "                     volume_col: str,\n",
    "                     vol_threshold: float,\n",
    "                     id_col: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Volume bars：累计成交量达到阈值就切一个 bar。\n",
    "    输出：\n",
    "      - bar_id（每个 id 内部）\n",
    "      - start_time/end_time/duration_sec\n",
    "      - open/high/low/close\n",
    "      - volume_sum, n_events\n",
    "      - ret（基于 close 的 log return）\n",
    "    \"\"\"\n",
    "    out = df.copy().sort_values([time_col] if id_col is None else [id_col, time_col])\n",
    "\n",
    "    def _one(g):\n",
    "        g = g.sort_values(time_col).copy()\n",
    "        vol = g[volume_col].fillna(0.0).astype(float).values\n",
    "        cumsum = np.cumsum(vol)\n",
    "        bar_id = (cumsum // float(vol_threshold)).astype(np.int64)\n",
    "        g[\"bar_id\"] = bar_id\n",
    "\n",
    "        grp = g.groupby(\"bar_id\", as_index=False)\n",
    "        res = grp.agg(\n",
    "            start_time=(time_col, \"first\"),\n",
    "            end_time=(time_col, \"last\"),\n",
    "            open=(price_col, \"first\"),\n",
    "            high=(price_col, \"max\"),\n",
    "            low=(price_col, \"min\"),\n",
    "            close=(price_col, \"last\"),\n",
    "            volume_sum=(volume_col, \"sum\"),\n",
    "            n_events=(volume_col, \"size\"),\n",
    "        )\n",
    "        res[\"duration_sec\"] = (res[\"end_time\"] - res[\"start_time\"]).dt.total_seconds()\n",
    "        res[\"ret\"] = np.log(res[\"close\"]).diff()\n",
    "        return res\n",
    "\n",
    "    if id_col is None:\n",
    "        return _one(out)\n",
    "\n",
    "    pieces = []\n",
    "    for _id, g in out.groupby(id_col, sort=False):\n",
    "        r = _one(g)\n",
    "        r[id_col] = _id\n",
    "        pieces.append(r)\n",
    "    return pd.concat(pieces, ignore_index=True).sort_values([id_col, \"end_time\"])\n",
    "\n",
    "\n",
    "def make_dollar_bars(df: pd.DataFrame,\n",
    "                   time_col: str,\n",
    "                   price_col: str,\n",
    "                   volume_col: str,\n",
    "                   dollar_threshold: float,\n",
    "                   id_col: str = None) -> pd.DataFrame:\n",
    "    \"\"\"Dollar bars：累计成交额达到阈值就切一个 bar。\"\"\"\n",
    "    out = df.copy().sort_values([time_col] if id_col is None else [id_col, time_col])\n",
    "    out[\"_dollar\"] = out[price_col].astype(float) * out[volume_col].astype(float)\n",
    "\n",
    "    def _one(g):\n",
    "        g = g.sort_values(time_col).copy()\n",
    "        amt = g[\"_dollar\"].fillna(0.0).values\n",
    "        cumsum = np.cumsum(amt)\n",
    "        bar_id = (cumsum // float(dollar_threshold)).astype(np.int64)\n",
    "        g[\"bar_id\"] = bar_id\n",
    "\n",
    "        grp = g.groupby(\"bar_id\", as_index=False)\n",
    "        res = grp.agg(\n",
    "            start_time=(time_col, \"first\"),\n",
    "            end_time=(time_col, \"last\"),\n",
    "            open=(price_col, \"first\"),\n",
    "            high=(price_col, \"max\"),\n",
    "            low=(price_col, \"min\"),\n",
    "            close=(price_col, \"last\"),\n",
    "            dollar_sum=(\"_dollar\", \"sum\"),\n",
    "            volume_sum=(volume_col, \"sum\"),\n",
    "            n_events=(volume_col, \"size\"),\n",
    "        )\n",
    "        res[\"duration_sec\"] = (res[\"end_time\"] - res[\"start_time\"]).dt.total_seconds()\n",
    "        res[\"ret\"] = np.log(res[\"close\"]).diff()\n",
    "        return res\n",
    "\n",
    "    if id_col is None:\n",
    "        return _one(out)\n",
    "\n",
    "    pieces = []\n",
    "    for _id, g in out.groupby(id_col, sort=False):\n",
    "        r = _one(g)\n",
    "        r[id_col] = _id\n",
    "        pieces.append(r)\n",
    "    return pd.concat(pieces, ignore_index=True).sort_values([id_col, \"end_time\"])\n",
    "\n",
    "\n",
    "# ====== 用法示例 ======\n",
    "# vbars = make_volume_bars(df, TIME_COL, price_col=\"price\", volume_col=\"size\", vol_threshold=10000, id_col=ID_COL)\n",
    "# dbars = make_dollar_bars(df, TIME_COL, price_col=\"price\", volume_col=\"size\", dollar_threshold=1_000_000, id_col=ID_COL)\n",
    "# vbars = add_calendar_features(vbars.rename(columns={\"end_time\": TIME_COL}), TIME_COL)  # 若想把 end_time 当 bar 时间戳\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc8d3e3",
   "metadata": {},
   "source": [
    "### 10.4 常见坑（写进检查清单里）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa10c2",
   "metadata": {},
   "source": [
    "- 盲目插值（对价格线性插值 tick）会制造“假交易”，扭曲波动与收益分布  \n",
    "- 直接把 tick 当均匀采样做 `shift(1)` 会混淆 1ms 与 5min 的历史信息；`gap_sec` 必须显式进入特征  \n",
    "- Time bars 里空桶不是噪声：`has_event` / `n_events`（或 `_is_missing`）必须保留  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
