{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd323a3",
   "metadata": {},
   "source": [
    "# Heavy Tail（长尾）与鲁棒处理：clip / Huber / Quantile / log1p / Box-Cox / Yeo-Johnson\n",
    "\n",
    "这个 notebook 用**可运行的最小示例**演示：\n",
    "1. 为什么 heavy tail 会让 **MSE/RMSE 被少数极端点主导**；\n",
    "2. 线性回归在离群点/长尾下为什么会不稳；\n",
    "3. 常见应对：**winsorize/clip、Huber、Quantile、log1p、Box-Cox、Yeo-Johnson**；\n",
    "4. 面试里建议同时汇报的指标与分桶验证方式。\n",
    "\n",
    "> 依赖：numpy, pandas, matplotlib, scikit-learn（常见面试环境都会有）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor, QuantileRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0671b21",
   "metadata": {},
   "source": [
    "## 1) 构造一个“主体分布正常 + 少量极端值”的回归数据\n",
    "\n",
    "我们用一个简单的线性关系：\n",
    "\\[\n",
    "y = 3x + \\epsilon\n",
    "\\]\n",
    "其中大多数噪声是正态，但会混入极少数极端噪声（重尾）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde97ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "x = np.random.normal(size=n)\n",
    "\n",
    "# 主体噪声：正态\n",
    "eps_main = np.random.normal(scale=1.0, size=n)\n",
    "\n",
    "# 极端噪声：少量点特别大（例如 t 分布/或直接乘大系数）\n",
    "is_tail = np.random.rand(n) < 0.02  # 2% 极端点\n",
    "eps_tail = np.random.normal(scale=25.0, size=n)  # 极端波动\n",
    "\n",
    "eps = np.where(is_tail, eps_tail, eps_main)\n",
    "y = 3.0 * x + eps\n",
    "\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "df = pd.DataFrame({\"x\": x, \"y\": y, \"is_tail\": is_tail})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe36e2a5",
   "metadata": {},
   "source": [
    "### 看一下分布与极端点比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818b24b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(y, bins=80)\n",
    "plt.title(\"Distribution of y (heavy-tailed)\")\n",
    "plt.xlabel(\"y\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n",
    "\n",
    "tail_share = df[\"is_tail\"].mean()\n",
    "tail_share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44f0ac",
   "metadata": {},
   "source": [
    "## 2) 为什么 MSE 会被少数点主导？（数值演示）\n",
    "\n",
    "我们先训练一个普通最小二乘线性回归（OLS），然后把误差贡献按样本排序，看看 top 1% 样本贡献了多少 MSE。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, tail_train, tail_test = train_test_split(\n",
    "    X, y, is_tail, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "ols = LinearRegression().fit(X_train, y_train)\n",
    "pred = ols.predict(X_test)\n",
    "\n",
    "resid = y_test - pred\n",
    "sq = resid**2\n",
    "\n",
    "mse = sq.mean()\n",
    "rmse = math.sqrt(mse)\n",
    "mae = np.abs(resid).mean()\n",
    "medae = np.median(np.abs(resid))\n",
    "\n",
    "mse, rmse, mae, medae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ddd625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE 贡献占比：按平方误差从大到小排序\n",
    "sq_sorted = np.sort(sq)[::-1]\n",
    "cum = np.cumsum(sq_sorted) / np.sum(sq_sorted)\n",
    "\n",
    "def share_of_top(p):\n",
    "    k = int(len(sq_sorted) * p)\n",
    "    return float(cum[k-1])\n",
    "\n",
    "for p in [0.001, 0.005, 0.01, 0.05]:\n",
    "    print(f\"Top {p*100:.1f}% samples contribute {share_of_top(p)*100:.1f}% of total squared error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8212e0fa",
   "metadata": {},
   "source": [
    "你会看到：**很少数的点贡献了绝大部分的平方误差**。\n",
    "这就是 heavy tail 下“优化 MSE = 追着极端点跑”的根源。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879fb66a",
   "metadata": {},
   "source": [
    "## 3) 线性模型为什么不稳：对极端点敏感\n",
    "\n",
    "我们对比：\n",
    "- OLS（最小二乘）\n",
    "- HuberRegressor（鲁棒损失）\n",
    "- QuantileRegressor（分位数回归，默认 τ=0.5 对应中位数回归）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"OLS (MSE)\": LinearRegression(),\n",
    "    \"Huber\": HuberRegressor(epsilon=1.35),\n",
    "    \"Quantile (tau=0.5)\": QuantileRegressor(quantile=0.5, alpha=0.0, solver=\"highs\"),\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, m in models.items():\n",
    "    m.fit(X_train, y_train)\n",
    "    p = m.predict(X_test)\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"RMSE\": mean_squared_error(y_test, p, squared=False),\n",
    "        \"MAE\": mean_absolute_error(y_test, p),\n",
    "        \"MedAE\": median_absolute_error(y_test, p),\n",
    "        \"coef\": float(m.coef_[0]) if hasattr(m, \"coef_\") else np.nan,\n",
    "        \"intercept\": float(m.intercept_) if hasattr(m, \"intercept_\") else np.nan,\n",
    "    })\n",
    "\n",
    "pd.DataFrame(results).sort_values(\"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd3ad4",
   "metadata": {},
   "source": [
    "### 分桶看误差：主体 vs 尾部\n",
    "把测试集按是否“极端点”分组，看看误差差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5a04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"RMSE\": mean_squared_error(y_true, y_pred, squared=False),\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MedAE\": median_absolute_error(y_true, y_pred),\n",
    "        \"n\": len(y_true),\n",
    "    }\n",
    "\n",
    "bucket_rows = []\n",
    "for name, m in models.items():\n",
    "    p = m.predict(X_test)\n",
    "    bucket_rows.append({\"model\": name, \"bucket\": \"ALL\", **metrics(y_test, p)})\n",
    "    bucket_rows.append({\"model\": name, \"bucket\": \"TAIL\", **metrics(y_test[tail_test], p[tail_test])})\n",
    "    bucket_rows.append({\"model\": name, \"bucket\": \"NON-TAIL\", **metrics(y_test[~tail_test], p[~tail_test])})\n",
    "\n",
    "pd.DataFrame(bucket_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2cee1",
   "metadata": {},
   "source": [
    "## 4) winsorize / clip（截尾/压尾）\n",
    "\n",
    "做法：把 y（或某些特征）超过分位数阈值的部分压到阈值上。\n",
    "这里演示在训练集上对 y 做 winsorize，然后再拟合 OLS（只示意，实际你也可对特征做）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb30f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize_by_quantile(arr, q_low=0.005, q_high=0.995):\n",
    "    lo, hi = np.quantile(arr, [q_low, q_high])\n",
    "    return np.clip(arr, lo, hi), lo, hi\n",
    "\n",
    "y_train_clip, lo, hi = winsorize_by_quantile(y_train, 0.005, 0.995)\n",
    "\n",
    "ols_clip = LinearRegression().fit(X_train, y_train_clip)\n",
    "p_clip = ols_clip.predict(X_test)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"model\": \"OLS raw y\", **metrics(y_test, ols.predict(X_test))},\n",
    "    {\"model\": \"OLS winsorized y_train\", **metrics(y_test, p_clip)},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2bbfc",
   "metadata": {},
   "source": [
    "## 5) log1p / Box-Cox / Yeo-Johnson：分布变换压长尾\n",
    "\n",
    "### 5.1 log1p（适用于 y ≥ 0）\n",
    "- 变换：`z = log(1 + y)`\n",
    "- 预测后还原：`y_hat = exp(z_hat) - 1`\n",
    "\n",
    "注意：如果 y 有负值，log1p 需要先 shift 或改用 Yeo-Johnson。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc316890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了演示 log1p，这里构造一个非负目标（常见如成交量/负载/金额）\n",
    "y_pos = np.exp(3.0 * x + np.random.normal(scale=0.8, size=n))  # 正值且长尾\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y_pos, test_size=0.3, random_state=0)\n",
    "\n",
    "# baseline：直接回归原尺度\n",
    "ols2 = LinearRegression().fit(X_train2, y_train2)\n",
    "p2 = ols2.predict(X_test2)\n",
    "\n",
    "# log1p：在 log 空间回归\n",
    "z_train = np.log1p(y_train2)\n",
    "ols_log = LinearRegression().fit(X_train2, z_train)\n",
    "z_pred = ols_log.predict(X_test2)\n",
    "p_log = np.expm1(z_pred)\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"model\": \"OLS on y\", **metrics(y_test2, p2)},\n",
    "    {\"model\": \"OLS on log1p(y) then invert\", **metrics(y_test2, p_log)},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1975ff9",
   "metadata": {},
   "source": [
    "### 5.2 Box-Cox（要求严格 y > 0）与 Yeo-Johnson（允许 y≤0）\n",
    "\n",
    "sklearn 的 `PowerTransformer` 支持：\n",
    "- `method=\"box-cox\"`：需要 y > 0\n",
    "- `method=\"yeo-johnson\"`：y 可以为任意实数\n",
    "\n",
    "下面展示对**回归目标 y**做变换（也可对特征做）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35821af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对上面的 y_pos (严格>0) 做 Box-Cox\n",
    "pt_bc = PowerTransformer(method=\"box-cox\", standardize=True)\n",
    "y_bc_train = pt_bc.fit_transform(y_train2.reshape(-1, 1)).ravel()\n",
    "\n",
    "m_bc = LinearRegression().fit(X_train2, y_bc_train)\n",
    "y_bc_pred = pt_bc.inverse_transform(m_bc.predict(X_test2).reshape(-1, 1)).ravel()\n",
    "\n",
    "# 对原来的 y（含负值）做 Yeo-Johnson\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "pt_yj = PowerTransformer(method=\"yeo-johnson\", standardize=True)\n",
    "y_yj_train = pt_yj.fit_transform(y_train3.reshape(-1, 1)).ravel()\n",
    "\n",
    "m_yj = LinearRegression().fit(X_train3, y_yj_train)\n",
    "y_yj_pred = pt_yj.inverse_transform(m_yj.predict(X_test3).reshape(-1, 1)).ravel()\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"model\": \"Box-Cox on y_pos then invert\", **metrics(y_test2, y_bc_pred)},\n",
    "    {\"model\": \"Yeo-Johnson on y then invert\", **metrics(y_test3, y_yj_pred)},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0cb2b4",
   "metadata": {},
   "source": [
    "## 6) 面试里建议的“注意事项清单”\n",
    "\n",
    "- **先定位 heavy tail 在哪**：是 y 还是 X？两者处理方式不同。\n",
    "- 不要只报一个指标：至少 `RMSE + MAE（或 MedAE）`，并**按分位数/桶**报告误差。\n",
    "- heavy tail 下用 MSE 训练要小心：可试 `Huber / Quantile / MAE` 或做 `clip/transform`。\n",
    "- `log1p` 只适用于 **y≥0**；`Box-Cox` 需要 **y>0**；`Yeo-Johnson` 可处理 **负值**。\n",
    "- 变换后记得可逆：预测要 **inverse_transform** 回原尺度再汇报业务指标。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
