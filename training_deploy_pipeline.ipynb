{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Dataset Interview：训练与部署安排（自用速查）\n\n**目标**：在同一套可复现实验管线下，训练/比较多个模型，输出稳定的结论与可落地的部署接口。\n\n- 所有模型共享：同一数据切分、同一特征工程、同一评估口径、同一预算\n- 重点检查：泄露（leakage）、时间漂移（drift）、稳定性（by time slice / by group）\n- 交付物：leaderboard、关键图、最终模型 artifact（pipeline + metadata）、推理函数\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. 运行环境与全局约束\n- 仅依赖：`numpy/pandas/scikit-learn/matplotlib`（不额外安装第三方包）\n- 时间序列任务：严格按时间顺序切分；任何 `fit`（imputer/scaler/encoder）只在训练折上发生\n- 结果可复现：固定随机种子；实验日志落盘\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# ===== Imports & Global Seed =====\nimport os\nimport json\nimport time\nimport math\nimport random\nimport warnings\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, roc_auc_score, log_loss\nfrom sklearn.model_selection import ParameterGrid\n\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\nfrom sklearn.svm import SVR, SVC\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nRUN_DIR = Path(\"runs\") / time.strftime(\"%Y%m%d_%H%M%S\")\nRUN_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"Run dir:\", RUN_DIR.resolve())\nprint(\"Python:\", os.sys.version.split()[0])\nprint(\"Numpy:\", np.__version__, \"Pandas:\", pd.__version__)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. 数据读取与字段定义\n这里统一把原始数据映射成：\n- `time_col`：时间列（可排序）\n- `target_col`：标签\n- 可选：`group_col`（ticker / id / meter_id 等）\n\n所有下游模块只依赖这三个概念。"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# ===== Load data =====\n# TODO: 替换为实际读取逻辑\n# df = pd.read_parquet(\"data/train.parquet\")  # 或 read_csv / read_feather 等\ndf = None\n\ntime_col = \"timestamp\"     # TODO\ntarget_col = \"y\"           # TODO\ngroup_col = None           # e.g., \"ticker\"；无则 None\n\n# ===== Basic sanity checks =====\ndef assert_columns_exist(df: pd.DataFrame, cols):\n    missing = [c for c in cols if c is not None and c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing columns: {missing}\")\n\n# assert_columns_exist(df, [time_col, target_col, group_col])\n\n# df[time_col] = pd.to_datetime(df[time_col])  # 视数据类型而定\n# df = df.sort_values(time_col).reset_index(drop=True)\n\n# print(df.shape)\n# display(df.head(3))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. 泄露检查清单（快速过一遍）\n- 特征中是否包含未来信息（例如未来收益、未来均值、未来统计窗口）\n- 是否在全量数据上 fit 了 scaler / encoder / imputer\n- 训练/验证是否按时间交叉（shuffle 造成穿越）\n- 同一实体跨时间重复出现时，是否出现“未来同组信息泄露”\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef leakage_sanity_report(df: pd.DataFrame, time_col: str, target_col: str):\n    # 只做轻量检查：空值比例、重复、目标是否异常\n    rep = {}\n    rep[\"n_rows\"] = int(df.shape[0])\n    rep[\"n_cols\"] = int(df.shape[1])\n    rep[\"target_nan_frac\"] = float(df[target_col].isna().mean())\n    rep[\"time_is_monotone\"] = bool(pd.Series(df[time_col]).is_monotonic_increasing)\n    rep[\"dup_row_frac\"] = float(df.duplicated().mean())\n    return rep\n\n# print(leakage_sanity_report(df, time_col, target_col))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. 切分：Train / Valid / Test + Walk-forward\n时序任务默认：\n- `test`：最后一段时间\n- `valid`：倒数第二段时间\n- `train`：更早的数据\n\n同时准备 walk-forward folds，用于更稳健的模型对比。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n@dataclass\nclass SplitConfig:\n    valid_frac: float = 0.15\n    test_frac: float = 0.15\n    min_train_size: int = 1000\n\nsplit_cfg = SplitConfig()\n\ndef time_based_split(df: pd.DataFrame, time_col: str, valid_frac: float, test_frac: float):\n    n = df.shape[0]\n    n_test = int(round(n * test_frac))\n    n_valid = int(round(n * valid_frac))\n    n_train = n - n_valid - n_test\n    if n_train <= 0:\n        raise ValueError(\"Split fractions too large.\")\n    idx_train = np.arange(0, n_train)\n    idx_valid = np.arange(n_train, n_train + n_valid)\n    idx_test  = np.arange(n_train + n_valid, n)\n    return idx_train, idx_valid, idx_test\n\ndef walk_forward_folds(df: pd.DataFrame, time_col: str, n_folds: int = 5, valid_size_frac: float = 0.1, min_train_size: int = 1000):\n    # expanding window; each fold uses a contiguous validation block after train block\n    n = df.shape[0]\n    valid_size = int(round(n * valid_size_frac))\n    valid_size = max(valid_size, 1)\n    # compute fold endpoints\n    folds = []\n    # ensure last fold ends before final test chunk if test exists (此处不强制，按需要裁剪)\n    step = (n - min_train_size - valid_size) // max(n_folds, 1)\n    step = max(step, 1)\n    for k in range(n_folds):\n        train_end = min_train_size + k * step\n        valid_start = train_end\n        valid_end = min(valid_start + valid_size, n)\n        if valid_end - valid_start < 1:\n            break\n        train_idx = np.arange(0, train_end)\n        valid_idx = np.arange(valid_start, valid_end)\n        folds.append((train_idx, valid_idx))\n        if valid_end >= n:\n            break\n    return folds\n\n# idx_train, idx_valid, idx_test = time_based_split(df, time_col, split_cfg.valid_frac, split_cfg.test_frac)\n# folds = walk_forward_folds(df, time_col, n_folds=5, valid_size_frac=0.1, min_train_size=split_cfg.min_train_size)\n# print(\"TVT sizes:\", len(idx_train), len(idx_valid), len(idx_test))\n# print(\"n_folds:\", len(folds))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. 特征工程骨架：把所有处理塞进 sklearn Pipeline\n原则：\n- 对数值/类别列分别处理\n- 后续所有模型都复用同一套 preprocessing\n- 时序特征（lag/rolling）在进入 pipeline 前生成（避免在 transform 时拿到未来）\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# ===== Feature columns (TODO: 由 df 自动推断或手动列出) =====\nnumeric_cols = []      # TODO\ncategorical_cols = []  # TODO\n\n# ===== Optional: create time features (calendar) =====\ndef add_time_features(df: pd.DataFrame, time_col: str):\n    out = df.copy()\n    t = pd.to_datetime(out[time_col])\n    out[\"hour\"] = t.dt.hour\n    out[\"dow\"] = t.dt.dayofweek\n    out[\"month\"] = t.dt.month\n    out[\"day\"] = t.dt.day\n    return out\n\n# ===== Optional: lag/rolling features (strictly past) =====\ndef add_lag_rolling_features(df: pd.DataFrame, time_col: str, group_col: str | None, base_cols: list[str],\n                             lags=(1, 2, 5), rolls=(5, 20)):\n    out = df.copy()\n    out = out.sort_values([c for c in [group_col, time_col] if c is not None]).copy()\n    gb = out.groupby(group_col, sort=False) if group_col else [(None, out)]\n    if group_col:\n        # groupby object\n        for col in base_cols:\n            for L in lags:\n                out[f\"{col}_lag{L}\"] = gb[col].shift(L)\n            for W in rolls:\n                out[f\"{col}_rollmean{W}\"] = gb[col].shift(1).rolling(W).mean()\n                out[f\"{col}_rollstd{W}\"]  = gb[col].shift(1).rolling(W).std()\n    else:\n        for col in base_cols:\n            for L in lags:\n                out[f\"{col}_lag{L}\"] = out[col].shift(L)\n            for W in rolls:\n                out[f\"{col}_rollmean{W}\"] = out[col].shift(1).rolling(W).mean()\n                out[f\"{col}_rollstd{W}\"]  = out[col].shift(1).rolling(W).std()\n    return out\n\n# ===== Preprocess pipeline =====\nnumeric_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", RobustScaler(with_centering=True))  # 或 StandardScaler\n])\n\ncategorical_pipe = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_pipe, numeric_cols),\n        (\"cat\", categorical_pipe, categorical_cols),\n    ],\n    remainder=\"drop\",\n    verbose_feature_names_out=False\n)\n\ndef make_xy(df: pd.DataFrame, target_col: str):\n    X = df.drop(columns=[target_col])\n    y = df[target_col].values\n    return X, y\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. 任务类型与指标\n回归与分类分别跑一套指标。面试现场只挑一套主指标讲清楚，其他指标做 sanity check。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n@dataclass\nclass TaskConfig:\n    task_type: str = \"regression\"  # \"classification\"\n    main_metric: str = \"mae\"       # regression: \"mae\"/\"rmse\"; classification: \"auc\"/\"logloss\"\n\ntask_cfg = TaskConfig()\n\ndef eval_metrics(y_true, y_pred, task_type: str):\n    out = {}\n    if task_type == \"regression\":\n        out[\"mae\"] = float(mean_absolute_error(y_true, y_pred))\n        out[\"rmse\"] = float(mean_squared_error(y_true, y_pred, squared=False))\n    else:\n        # y_pred: probability for positive class\n        # 需要保证 y_true ∈ {0,1}\n        out[\"auc\"] = float(roc_auc_score(y_true, y_pred)) if len(np.unique(y_true)) > 1 else float(\"nan\")\n        # log_loss 需要 [p, 1-p] or probability; sklearn accepts prob for positive with labels?\n        try:\n            out[\"logloss\"] = float(log_loss(y_true, y_pred, eps=1e-15))\n        except Exception:\n            out[\"logloss\"] = float(\"nan\")\n    return out\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. 模型候选集合（Model Zoo）\n每个模型只改 estimator，本体共享同一个 `preprocess`。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef model_zoo(task_type: str):\n    if task_type == \"regression\":\n        models = {\n            \"baseline_ridge\": Ridge(random_state=SEED) if \"random_state\" in Ridge().get_params() else Ridge(),\n            \"lasso\": Lasso(max_iter=5000, random_state=SEED) if \"random_state\" in Lasso().get_params() else Lasso(max_iter=5000),\n            \"elasticnet\": ElasticNet(max_iter=5000, random_state=SEED) if \"random_state\" in ElasticNet().get_params() else ElasticNet(max_iter=5000),\n            \"rf\": RandomForestRegressor(n_estimators=300, random_state=SEED, n_jobs=-1),\n            \"hgb\": HistGradientBoostingRegressor(random_state=SEED),\n            \"svr\": SVR(),\n        }\n    else:\n        models = {\n            \"logreg\": LogisticRegression(max_iter=2000, n_jobs=-1),\n            \"rf\": RandomForestClassifier(n_estimators=400, random_state=SEED, n_jobs=-1),\n            \"hgb\": HistGradientBoostingClassifier(random_state=SEED),\n            \"svc\": SVC(probability=True, random_state=SEED),\n        }\n    return models\n\nmodels = model_zoo(task_cfg.task_type)\nlist(models.keys())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. 调参预算（统一预算，公平对比）\n这里用 `ParameterGrid` 手动小网格：快、可控、debug 友好。\n每个模型控制在 5~20 组以内。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef param_grids(task_type: str):\n    if task_type == \"regression\":\n        grids = {\n            \"baseline_ridge\": {\"model__alpha\": [0.1, 1.0, 10.0]},\n            \"lasso\": {\"model__alpha\": [1e-4, 1e-3, 1e-2]},\n            \"elasticnet\": {\"model__alpha\": [1e-4, 1e-3], \"model__l1_ratio\": [0.2, 0.5, 0.8]},\n            \"rf\": {\"model__max_depth\": [None, 8, 16], \"model__min_samples_leaf\": [1, 5]},\n            \"hgb\": {\"model__learning_rate\": [0.05, 0.1], \"model__max_depth\": [None, 6], \"model__max_leaf_nodes\": [31, 63]},\n            \"svr\": {\"model__C\": [0.5, 1.0, 2.0], \"model__gamma\": [\"scale\", \"auto\"]},\n        }\n    else:\n        grids = {\n            \"logreg\": {\"model__C\": [0.5, 1.0, 2.0]},\n            \"rf\": {\"model__max_depth\": [None, 8, 16], \"model__min_samples_leaf\": [1, 5]},\n            \"hgb\": {\"model__learning_rate\": [0.05, 0.1], \"model__max_depth\": [None, 6], \"model__max_leaf_nodes\": [31, 63]},\n            \"svc\": {\"model__C\": [0.5, 1.0, 2.0], \"model__gamma\": [\"scale\", \"auto\"]},\n        }\n    return grids\n\ngrids = param_grids(task_cfg.task_type)\n{ k: len(list(ParameterGrid(v))) for k,v in grids.items() }\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. 统一训练-验证循环（Walk-forward）\n输出：\n- 每个 (model, params) 的 fold 分数、均值、方差\n- 训练耗时\n- 选出最优配置并在 TVT 上复验\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef fit_predict(est: Pipeline, X_train, y_train, X_valid):\n    est.fit(X_train, y_train)\n    if task_cfg.task_type == \"classification\":\n        # 统一用 positive class prob\n        if hasattr(est, \"predict_proba\"):\n            p = est.predict_proba(X_valid)[:, 1]\n            return p\n        # fallback\n        s = est.decision_function(X_valid)\n        # map scores to (0,1) via sigmoid as a rough surrogate\n        p = 1.0 / (1.0 + np.exp(-s))\n        return p\n    else:\n        return est.predict(X_valid)\n\ndef score_on_folds(df, folds, pipe_template, task_type):\n    rows = []\n    X_all, y_all = make_xy(df, target_col)\n    for fold_id, (tr_idx, va_idx) in enumerate(folds):\n        X_tr, y_tr = X_all.iloc[tr_idx], y_all[tr_idx]\n        X_va, y_va = X_all.iloc[va_idx], y_all[va_idx]\n        t0 = time.time()\n        y_hat = fit_predict(pipe_template, X_tr, y_tr, X_va)\n        dt = time.time() - t0\n        m = eval_metrics(y_va, y_hat, task_type)\n        m[\"fold\"] = fold_id\n        m[\"train_size\"] = int(len(tr_idx))\n        m[\"valid_size\"] = int(len(va_idx))\n        m[\"fit_time_sec\"] = float(dt)\n        rows.append(m)\n    return pd.DataFrame(rows)\n\ndef run_model_search(df, folds, preprocess, models, grids, task_type, main_metric):\n    results = []\n    for name, est in models.items():\n        grid = grids.get(name, {})\n        for params in ParameterGrid(grid):\n            pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", clone(est))])\n            pipe.set_params(**params)\n            fold_df = score_on_folds(df, folds, pipe, task_type)\n            mean_score = float(fold_df[main_metric].mean())\n            std_score = float(fold_df[main_metric].std(ddof=0))\n            total_time = float(fold_df[\"fit_time_sec\"].sum())\n            results.append({\n                \"model\": name,\n                \"params\": json.dumps(params, sort_keys=True),\n                f\"{main_metric}_mean\": mean_score,\n                f\"{main_metric}_std\": std_score,\n                \"total_fit_time_sec\": total_time,\n                \"n_folds\": int(fold_df.shape[0]),\n            })\n            # 记录每个 fold 的明细\n            fold_path = RUN_DIR / f\"folds__{name}__{hash(json.dumps(params, sort_keys=True))}.csv\"\n            fold_df.to_csv(fold_path, index=False)\n    res_df = pd.DataFrame(results)\n    # main_metric: 回归越小越好（mae/rmse），分类 auc 越大越好，logloss 越小越好\n    if task_type == \"classification\" and main_metric == \"auc\":\n        res_df = res_df.sort_values(by=f\"{main_metric}_mean\", ascending=False)\n    else:\n        res_df = res_df.sort_values(by=f\"{main_metric}_mean\", ascending=True)\n    return res_df\n\n# ===== Example run (uncomment when df is ready) =====\n# folds = walk_forward_folds(df, time_col, n_folds=5, valid_size_frac=0.1, min_train_size=split_cfg.min_train_size)\n# leaderboard = run_model_search(df, folds, preprocess, models, grids, task_cfg.task_type, task_cfg.main_metric)\n# leaderboard.to_csv(RUN_DIR / \"leaderboard_cv.csv\", index=False)\n# display(leaderboard.head(20))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. 在 TVT 上复验最优模型 + 最终训练\n步骤：\n1) 从 leaderboard 取 top1（或 topK）\n2) 在 train 上 fit，在 valid 上评估（sanity）\n3) train+valid 合并重新 fit\n4) 在 test 上给最终分数\n5) 落盘：pipeline + config + 特征名\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nimport pickle\n\ndef refit_and_eval_tvt(df, idx_train, idx_valid, idx_test, preprocess, model_est, params: dict, task_type: str):\n    # train\n    X_all, y_all = make_xy(df, target_col)\n    X_tr, y_tr = X_all.iloc[idx_train], y_all[idx_train]\n    X_va, y_va = X_all.iloc[idx_valid], y_all[idx_valid]\n    X_te, y_te = X_all.iloc[idx_test],  y_all[idx_test]\n\n    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", clone(model_est))])\n    pipe.set_params(**params)\n\n    # fit on train, eval on valid\n    y_hat_va = fit_predict(pipe, X_tr, y_tr, X_va)\n    va_metrics = eval_metrics(y_va, y_hat_va, task_type)\n\n    # fit on train+valid, eval on test\n    trva_idx = np.concatenate([idx_train, idx_valid])\n    X_trva, y_trva = X_all.iloc[trva_idx], y_all[trva_idx]\n    pipe.fit(X_trva, y_trva)\n    if task_type == \"classification\":\n        y_hat_te = pipe.predict_proba(X_te)[:, 1] if hasattr(pipe, \"predict_proba\") else pipe.decision_function(X_te)\n        if y_hat_te.ndim != 1:\n            y_hat_te = np.asarray(y_hat_te).ravel()\n        if task_cfg.main_metric != \"auc\":  # if decision_function, map sigmoid for logloss\n            y_hat_te = 1.0 / (1.0 + np.exp(-y_hat_te))\n    else:\n        y_hat_te = pipe.predict(X_te)\n    te_metrics = eval_metrics(y_te, y_hat_te, task_type)\n\n    return pipe, va_metrics, te_metrics, (y_te, y_hat_te)\n\n# ===== Example usage =====\n# idx_train, idx_valid, idx_test = time_based_split(df, time_col, split_cfg.valid_frac, split_cfg.test_frac)\n# best = leaderboard.iloc[0]\n# best_name = best[\"model\"]\n# best_params = json.loads(best[\"params\"])\n# pipe_final, va_m, te_m, (y_te, yhat_te) = refit_and_eval_tvt(\n#     df, idx_train, idx_valid, idx_test,\n#     preprocess, models[best_name], best_params, task_cfg.task_type\n# )\n# print(\"VALID:\", va_m)\n# print(\"TEST :\", te_m)\n\n# ===== Save artifact =====\n# artifact = {\n#     \"time_col\": time_col,\n#     \"target_col\": target_col,\n#     \"group_col\": group_col,\n#     \"task_cfg\": asdict(task_cfg),\n#     \"split_cfg\": asdict(split_cfg),\n#     \"best_model\": best_name,\n#     \"best_params\": best_params,\n#     \"valid_metrics\": va_m,\n#     \"test_metrics\": te_m,\n#     \"run_dir\": str(RUN_DIR),\n# }\n# with open(RUN_DIR / \"artifact_meta.json\", \"w\") as f:\n#     json.dump(artifact, f, indent=2)\n# with open(RUN_DIR / \"pipeline.pkl\", \"wb\") as f:\n#     pickle.dump(pipe_final, f)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. 稳定性：按时间切片评估（核心汇报图）\n输出：\n- 每个时间桶的指标（例如按月/按周/按天）\n- 观察 drift：是否越到后面越差\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef time_slice_scores(df, idx, y_true, y_pred, time_col: str, freq: str, task_type: str):\n    # freq: \"W\" / \"M\" / \"D\" etc\n    t = pd.to_datetime(df.iloc[idx][time_col]).dt.to_period(freq).astype(str)\n    tmp = pd.DataFrame({\"bucket\": t.values, \"y\": y_true, \"pred\": y_pred})\n    out_rows = []\n    for b, g in tmp.groupby(\"bucket\", sort=True):\n        m = eval_metrics(g[\"y\"].values, g[\"pred\"].values, task_type)\n        m[\"bucket\"] = b\n        m[\"n\"] = int(g.shape[0])\n        out_rows.append(m)\n    out = pd.DataFrame(out_rows).sort_values(\"bucket\")\n    return out\n\ndef plot_time_metric(ts_df: pd.DataFrame, metric: str):\n    plt.figure()\n    plt.plot(ts_df[\"bucket\"], ts_df[metric])\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.xlabel(\"time bucket\")\n    plt.ylabel(metric)\n    plt.title(f\"{metric} by time bucket\")\n    plt.tight_layout()\n    plt.show()\n\n# ===== Example =====\n# ts = time_slice_scores(df, idx_test, y_te, yhat_te, time_col=time_col, freq=\"W\", task_type=task_cfg.task_type)\n# ts.to_csv(RUN_DIR / \"test_time_slices.csv\", index=False)\n# display(ts.head())\n# plot_time_metric(ts, task_cfg.main_metric)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11. 误差诊断：残差/分位数/尾部\n回归：\n- residual 分布\n- pred vs true\n- 绝对误差分位数\n\n分类：\n- 分桶校准（简单版）\n- top-decile lift（按概率排序）\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef regression_diagnostics(y_true, y_pred):\n    resid = y_true - y_pred\n\n    plt.figure()\n    plt.hist(resid, bins=50)\n    plt.title(\"Residual histogram\")\n    plt.xlabel(\"y - pred\")\n    plt.ylabel(\"count\")\n    plt.tight_layout()\n    plt.show()\n\n    plt.figure()\n    plt.scatter(y_pred, y_true, s=6)\n    plt.title(\"Pred vs True\")\n    plt.xlabel(\"pred\")\n    plt.ylabel(\"true\")\n    plt.tight_layout()\n    plt.show()\n\n    ae = np.abs(resid)\n    qs = np.quantile(ae, [0.5, 0.8, 0.9, 0.95, 0.99])\n    return {\"abs_err_quantiles\": qs.tolist()}\n\ndef classification_topk(y_true, p_pred, k_frac=0.1):\n    n = len(y_true)\n    k = max(int(n * k_frac), 1)\n    order = np.argsort(-p_pred)\n    top = y_true[order[:k]].mean()\n    base = y_true.mean()\n    lift = (top / base) if base > 0 else float(\"nan\")\n    return {\"top_frac\": k_frac, \"top_mean\": float(top), \"base_mean\": float(base), \"lift\": float(lift)}\n\n# ===== Example =====\n# if task_cfg.task_type == \"regression\":\n#     diag = regression_diagnostics(y_te, yhat_te)\n#     print(diag)\n# else:\n#     print(classification_topk(y_te, yhat_te, 0.1))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 12. 模型解释（在树模型上更好讲）\n- HGB / RF：feature_importances_（若存在）\n- 线性模型：coef_（经过 one-hot 后会变长）\n\n这里只做轻量输出：Top-N 特征名 + 重要性。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\ndef get_feature_names(pipe: Pipeline):\n    pre = pipe.named_steps[\"preprocess\"]\n    try:\n        names = pre.get_feature_names_out()\n        return list(names)\n    except Exception:\n        return None\n\ndef top_feature_importance(pipe: Pipeline, topn: int = 30):\n    model = pipe.named_steps[\"model\"]\n    names = get_feature_names(pipe)\n    if hasattr(model, \"feature_importances_\"):\n        imp = model.feature_importances_\n    elif hasattr(model, \"coef_\"):\n        coef = model.coef_\n        imp = np.abs(coef).ravel()\n    else:\n        return None\n\n    if names is None:\n        names = [f\"f{i}\" for i in range(len(imp))]\n    s = pd.DataFrame({\"feature\": names, \"importance\": imp})\n    s = s.sort_values(\"importance\", ascending=False).head(topn)\n    return s\n\n# ===== Example =====\n# fi = top_feature_importance(pipe_final, topn=25)\n# if fi is not None:\n#     fi.to_csv(RUN_DIR / \"top_features.csv\", index=False)\n#     display(fi)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 13. 部署接口：离线 artifact + 在线推理函数\n交付目标：\n- `pipeline.pkl`：包含 preprocess + model\n- `artifact_meta.json`：记录字段、版本、指标、训练时间窗口\n- `predict(df_new)`：输入一批新数据，输出预测\n\n部署时的硬约束：\n- 输入数据必须包含训练时同名字段\n- 时间特征/lag 特征生成必须与训练一致（同一段代码复用）\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# ===== Load & Predict (deployment stub) =====\ndef load_pipeline(run_dir: str | Path):\n    run_dir = Path(run_dir)\n    with open(run_dir / \"pipeline.pkl\", \"rb\") as f:\n        pipe = pickle.load(f)\n    with open(run_dir / \"artifact_meta.json\", \"r\") as f:\n        meta = json.load(f)\n    return pipe, meta\n\ndef predict_batch(pipe: Pipeline, df_new: pd.DataFrame):\n    # df_new: 仅包含特征列（不要包含 target）\n    # 若训练用了 add_time_features / add_lag_rolling_features，这里必须同样处理\n    if task_cfg.task_type == \"classification\":\n        if hasattr(pipe, \"predict_proba\"):\n            return pipe.predict_proba(df_new)[:, 1]\n        s = pipe.decision_function(df_new)\n        return 1.0 / (1.0 + np.exp(-s))\n    else:\n        return pipe.predict(df_new)\n\n# ===== Example =====\n# pipe_loaded, meta = load_pipeline(RUN_DIR)\n# preds = predict_batch(pipe_loaded, df_new)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 14. 面试当天时间安排（从 9:00 到 18:00）\n### 9:00–10:00 读题 + 数据快速体检\n- 明确 label、时间字段、可用特征、不可用信息\n- 画数据时间轴：覆盖区间、缺失、异常\n\n### 10:00–12:00 Baseline + 第一版 Pipeline\n- TVT 切分 + 最基础特征 + Ridge/LogReg\n- 泄露排查：fit 是否只在 train\n\n### 12:00–14:30 扩展特征 + 树模型/Boosting\n- 加入 lag/rolling（如适用）\n- RF/HGB 小网格调参\n\n### 14:30–16:30 稳定性与诊断\n- 时间切片分数\n- 残差/尾部/子样本\n\n### 16:30–17:30 最终定型 + 落盘\n- 选模型：分数 + 稳定性 + 复杂度\n- 保存 pipeline.pkl + meta.json\n\n### 17:30–18:00 汇报稿\n- 问题定义、切分、特征、模型对比、诊断、最终选择与部署接口\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 15. 最终汇报模板（10 页以内）\n1) Problem & Target\n2) Data overview + leakage constraints\n3) Split scheme (time-based / walk-forward)\n4) Feature set v1 (baseline) + results\n5) Feature set v2 (time features / lag/rolling) + results\n6) Model comparison leaderboard\n7) Stability by time slices\n8) Diagnostics (residual / calibration / tail)\n9) Final model choice (trade-offs)\n10) Deployment: artifact + predict() + monitoring\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}