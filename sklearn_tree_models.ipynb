{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3440b0",
   "metadata": {},
   "source": [
    "# Sklearn 树模型训练与部署速查（Dataset Interview）\n",
    "\n",
    "更新时间：2026-01-29\n",
    "\n",
    "目标：拿到任意表格数据后，快速完成：数据切分 → 预处理 → 训练树模型（回归/分类） → 可靠评估（含时间序列） → 解释（重要性） → 产出可展示结果。\n",
    "\n",
    "约束：默认使用 Python + numpy/pandas + scikit-learn；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554a0a2",
   "metadata": {},
   "source": [
    "## 0. 环境与导入\n",
    "- 统一随机种子\n",
    "- 统一展示与日志\n",
    "- 统一指标与切分工具\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d44ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, roc_auc_score, average_precision_score, f1_score, log_loss\n",
    ")\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, RandomForestClassifier,\n",
    "    ExtraTreesRegressor, ExtraTreesClassifier,\n",
    "    GradientBoostingRegressor, GradientBoostingClassifier,\n",
    "    AdaBoostRegressor, AdaBoostClassifier\n",
    ")\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa: F401\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def safe_auc(y_true, y_score):\n",
    "    y_true = np.asarray(y_true)\n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return np.nan\n",
    "    return float(roc_auc_score(y_true, y_score))\n",
    "\n",
    "def timestamp():\n",
    "    return time.strftime(\"%Y%m%d_%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac58e579",
   "metadata": {},
   "source": [
    "## 1. 数据快速体检（5 分钟以内）\n",
    "输出：shape、缺失率、重复行、目标分布、时间字段提示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671af18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_profile(df: pd.DataFrame, target: Optional[str] = None, time_col: Optional[str] = None, n_head: int = 3):\n",
    "    print(\"shape:\", df.shape)\n",
    "    print(\"\\nhead:\")\n",
    "    display(df.head(n_head))\n",
    "    print(\"\\ndtypes:\")\n",
    "    display(df.dtypes.value_counts())\n",
    "    \n",
    "    miss = df.isna().mean().sort_values(ascending=False)\n",
    "    print(\"\\nmissing rate (top 20):\")\n",
    "    display(miss.head(20))\n",
    "    \n",
    "    dup = df.duplicated().mean()\n",
    "    print(f\"\\nduplicate row rate: {dup:.4f}\")\n",
    "    \n",
    "    if target is not None and target in df.columns:\n",
    "        y = df[target]\n",
    "        print(\"\\nTarget summary:\")\n",
    "        try:\n",
    "            display(y.describe())\n",
    "        except Exception:\n",
    "            display(y.head())\n",
    "        if y.dtype == \"O\" or y.nunique(dropna=True) <= 20:\n",
    "            print(\"\\nTarget value_counts (top 20):\")\n",
    "            display(y.value_counts().head(20))\n",
    "    \n",
    "    if time_col is None:\n",
    "        cand = [c for c in df.columns if re.search(r\"(date|time|timestamp)\", c, flags=re.I)]\n",
    "        if cand:\n",
    "            print(\"\\nPossible time columns:\", cand[:10])\n",
    "    else:\n",
    "        print(\"\\nTime column:\", time_col)\n",
    "\n",
    "def infer_task_type(y: pd.Series) -> str:\n",
    "    # 输出: \"regression\" / \"binary\" / \"multiclass\"\n",
    "    if y.dtype == \"O\":\n",
    "        nunique = y.nunique(dropna=True)\n",
    "        return \"binary\" if nunique == 2 else \"multiclass\"\n",
    "    nunique = y.nunique(dropna=True)\n",
    "    if nunique <= 2:\n",
    "        return \"binary\"\n",
    "    if nunique <= 20 and y.dropna().astype(int).equals(y.dropna()):\n",
    "        return \"multiclass\"\n",
    "    return \"regression\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707acd8e",
   "metadata": {},
   "source": [
    "## 2. 切分策略（非时序 / 时序）\n",
    "- 非时序：train_test_split\n",
    "- 时序：按时间排序后做 holdout；或 TimeSeriesSplit 做 CV\n",
    "\n",
    "泄漏自检：rolling/聚合 特征必须基于过去信息（例如 shift(1)）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b10fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_holdout(\n",
    "    df: pd.DataFrame,\n",
    "    target: str,\n",
    "    time_col: Optional[str] = None,\n",
    "    test_size: float = 0.2,\n",
    "    valid_size: float = 0.2,\n",
    "    shuffle: bool = True,\n",
    "    stratify: bool = False\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, pd.Series]:\n",
    "    # 返回：X_train, X_valid, X_test, y_train, y_valid, y_test\n",
    "    # 若 time_col 给定：按 time_col 排序并做顺序切分（无 shuffle）\n",
    "    # 否则：常规随机切分；分类任务可启用 stratify\n",
    "    \n",
    "    assert target in df.columns\n",
    "    df2 = df.copy()\n",
    "    y = df2.pop(target)\n",
    "\n",
    "    if time_col is not None:\n",
    "        df2 = df2.sort_values(time_col).reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "        n = len(df2)\n",
    "        n_test = int(round(n * test_size))\n",
    "        n_valid = int(round((n - n_test) * valid_size))\n",
    "        n_train = n - n_test - n_valid\n",
    "\n",
    "        X_train = df2.iloc[:n_train]\n",
    "        X_valid = df2.iloc[n_train:n_train + n_valid]\n",
    "        X_test  = df2.iloc[n_train + n_valid:]\n",
    "\n",
    "        y_train = y.iloc[:n_train]\n",
    "        y_valid = y.iloc[n_train:n_train + n_valid]\n",
    "        y_test  = y.iloc[n_train + n_valid:]\n",
    "        return X_train, X_valid, X_test, y_train, y_valid, y_test\n",
    "\n",
    "    strat = y if stratify else None\n",
    "    X_trv, X_test, y_trv, y_test = train_test_split(\n",
    "        df2, y, test_size=test_size, random_state=RANDOM_STATE, shuffle=shuffle, stratify=strat\n",
    "    )\n",
    "    strat2 = y_trv if stratify else None\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_trv, y_trv, test_size=valid_size, random_state=RANDOM_STATE, shuffle=shuffle, stratify=strat2\n",
    "    )\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf54610",
   "metadata": {},
   "source": [
    "## 3. 预处理与 Pipeline（树模型专用）\n",
    "树模型不依赖特征缩放；重点：缺失值 + 类别编码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa3ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocess(X: pd.DataFrame, categorical_cols: Optional[List[str]] = None):\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = [c for c in X.columns if X[c].dtype == \"O\" or str(X[c].dtype).startswith(\"category\")]\n",
    "    numeric_cols = [c for c in X.columns if c not in categorical_cols]\n",
    "\n",
    "    numeric_tf = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    ])\n",
    "    cat_tf = Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "    ])\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_tf, numeric_cols),\n",
    "            (\"cat\", cat_tf, categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "    return preprocess, numeric_cols, categorical_cols\n",
    "\n",
    "def get_feature_names(pipe: Pipeline) -> List[str]:\n",
    "    prep = pipe.named_steps[\"prep\"]\n",
    "    try:\n",
    "        return list(prep.get_feature_names_out())\n",
    "    except Exception:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c6d33c",
   "metadata": {},
   "source": [
    "## 4. 常用树模型与默认配置\n",
    "覆盖：DecisionTree / RandomForest / ExtraTrees / GradientBoosting / AdaBoost / HistGradientBoosting。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e21a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(task: str, model_name: str):\n",
    "    is_clf = task in (\"binary\", \"multiclass\")\n",
    "\n",
    "    if model_name == \"hgb\":\n",
    "        if is_clf:\n",
    "            return HistGradientBoostingClassifier(\n",
    "                learning_rate=0.05,\n",
    "                max_depth=6,\n",
    "                max_iter=400,\n",
    "                min_samples_leaf=50,\n",
    "                l2_regularization=0.1,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "        return HistGradientBoostingRegressor(\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            max_iter=400,\n",
    "            min_samples_leaf=50,\n",
    "            l2_regularization=0.1,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "\n",
    "    if model_name == \"rf\":\n",
    "        if is_clf:\n",
    "            return RandomForestClassifier(\n",
    "                n_estimators=400,\n",
    "                max_depth=None,\n",
    "                min_samples_leaf=5,\n",
    "                n_jobs=-1,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "        return RandomForestRegressor(\n",
    "            n_estimators=400,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=5,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "\n",
    "    if model_name == \"et\":\n",
    "        if is_clf:\n",
    "            return ExtraTreesClassifier(\n",
    "                n_estimators=600,\n",
    "                max_depth=None,\n",
    "                min_samples_leaf=2,\n",
    "                n_jobs=-1,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "        return ExtraTreesRegressor(\n",
    "            n_estimators=600,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=2,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "\n",
    "    if model_name == \"gbrt\":\n",
    "        if is_clf:\n",
    "            return GradientBoostingClassifier(\n",
    "                learning_rate=0.05,\n",
    "                n_estimators=300,\n",
    "                max_depth=3,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "        return GradientBoostingRegressor(\n",
    "            learning_rate=0.05,\n",
    "            n_estimators=300,\n",
    "            max_depth=3,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "\n",
    "    if model_name == \"ada\":\n",
    "        if is_clf:\n",
    "            return AdaBoostClassifier(\n",
    "                n_estimators=400,\n",
    "                learning_rate=0.05,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "        return AdaBoostRegressor(\n",
    "            n_estimators=400,\n",
    "            learning_rate=0.05,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "\n",
    "    if model_name == \"dt\":\n",
    "        if is_clf:\n",
    "            return DecisionTreeClassifier(\n",
    "                max_depth=8,\n",
    "                min_samples_leaf=20,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "        return DecisionTreeRegressor(\n",
    "            max_depth=8,\n",
    "                min_samples_leaf=20,\n",
    "                random_state=RANDOM_STATE\n",
    "        )\n",
    "\n",
    "    raise ValueError(f\"Unknown model_name={model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0186b",
   "metadata": {},
   "source": [
    "## 5. 训练与评估（Holdout）\n",
    "回归：MAE / RMSE / R2；分类：Accuracy / AUC / PR-AUC / F1 / LogLoss。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e61cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(task: str, y_true, y_pred, y_proba=None) -> Dict[str, float]:\n",
    "    out = {}\n",
    "    if task == \"regression\":\n",
    "        out[\"mae\"] = float(mean_absolute_error(y_true, y_pred))\n",
    "        out[\"rmse\"] = rmse(y_true, y_pred)\n",
    "        out[\"r2\"] = float(r2_score(y_true, y_pred))\n",
    "        return out\n",
    "\n",
    "    out[\"acc\"] = float(accuracy_score(y_true, y_pred))\n",
    "    out[\"f1_macro\"] = float(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "\n",
    "    if y_proba is not None:\n",
    "        if task == \"binary\":\n",
    "            out[\"auc\"] = safe_auc(y_true, y_proba)\n",
    "            out[\"pr_auc\"] = float(average_precision_score(y_true, y_proba)) if len(np.unique(y_true)) > 1 else np.nan\n",
    "            try:\n",
    "                out[\"logloss\"] = float(log_loss(y_true, np.c_[1 - y_proba, y_proba]))\n",
    "            except Exception:\n",
    "                out[\"logloss\"] = np.nan\n",
    "        else:\n",
    "            try:\n",
    "                out[\"logloss\"] = float(log_loss(y_true, y_proba))\n",
    "            except Exception:\n",
    "                out[\"logloss\"] = np.nan\n",
    "    return out\n",
    "\n",
    "def fit_one_model(\n",
    "    X_train, y_train, X_valid, y_valid,\n",
    "    task: str, model_name: str,\n",
    "    categorical_cols: Optional[List[str]] = None\n",
    ") -> Tuple[Pipeline, Dict[str, float]]:\n",
    "    preprocess, _, _ = build_preprocess(X_train, categorical_cols=categorical_cols)\n",
    "    model = make_model(task, model_name)\n",
    "    pipe = Pipeline(steps=[(\"prep\", preprocess), (\"model\", model)])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    yhat = pipe.predict(X_valid)\n",
    "    yproba = None\n",
    "    if task == \"binary\":\n",
    "        yproba = pipe.predict_proba(X_valid)[:, 1]\n",
    "    elif task == \"multiclass\":\n",
    "        yproba = pipe.predict_proba(X_valid)\n",
    "\n",
    "    metrics = evaluate(task, y_valid, yhat, yproba)\n",
    "    return pipe, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c469d3",
   "metadata": {},
   "source": [
    "## 6. 多模型对比（同一套切分/预处理）\n",
    "一次跑完常见模型，输出对比表。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50109f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_models(\n",
    "    X_train, y_train, X_valid, y_valid, task: str,\n",
    "    model_names=(\"hgb\", \"rf\", \"et\", \"gbrt\", \"dt\"),\n",
    "    categorical_cols: Optional[List[str]] = None\n",
    "):\n",
    "    rows = []\n",
    "    pipes = {}\n",
    "    for name in model_names:\n",
    "        pipe, m = fit_one_model(X_train, y_train, X_valid, y_valid, task, name, categorical_cols=categorical_cols)\n",
    "        pipes[name] = pipe\n",
    "        rows.append({\"model\": name, **m})\n",
    "    return pd.DataFrame(rows), pipes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67859eb4",
   "metadata": {},
   "source": [
    "## 7. 交叉验证（非时序 / 时序）\n",
    "时序：TimeSeriesSplit（不打乱）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_pipeline(\n",
    "    X: pd.DataFrame, y: pd.Series, task: str,\n",
    "    model_name: str = \"hgb\",\n",
    "    time_series: bool = False,\n",
    "    n_splits: int = 5,\n",
    "    categorical_cols: Optional[List[str]] = None\n",
    "):\n",
    "    preprocess, _, _ = build_preprocess(X, categorical_cols=categorical_cols)\n",
    "    model = make_model(task, model_name)\n",
    "    pipe = Pipeline(steps=[(\"prep\", preprocess), (\"model\", model)])\n",
    "\n",
    "    if time_series:\n",
    "        splitter = TimeSeriesSplit(n_splits=n_splits)\n",
    "        split_iter = splitter.split(X)\n",
    "    else:\n",
    "        if task in (\"binary\", \"multiclass\"):\n",
    "            splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "        else:\n",
    "            splitter = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_STATE)\n",
    "        split_iter = splitter.split(X, y)\n",
    "\n",
    "    fold_rows = []\n",
    "    for i, (tr_idx, va_idx) in enumerate(split_iter):\n",
    "        Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "        ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "        pipe.fit(Xtr, ytr)\n",
    "\n",
    "        yhat = pipe.predict(Xva)\n",
    "        yproba = None\n",
    "        if task == \"binary\":\n",
    "            yproba = pipe.predict_proba(Xva)[:, 1]\n",
    "        elif task == \"multiclass\":\n",
    "            yproba = pipe.predict_proba(Xva)\n",
    "\n",
    "        m = evaluate(task, yva, yhat, yproba)\n",
    "        fold_rows.append({\"fold\": i, **m})\n",
    "\n",
    "    dfm = pd.DataFrame(fold_rows)\n",
    "    summary = dfm.drop(columns=[\"fold\"]).agg([\"mean\", \"std\"]).T\n",
    "    return dfm, summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a953a62",
   "metadata": {},
   "source": [
    "## 8. 特征重要性（Permutation Importance）\n",
    "输出：TopK 特征与重要性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importance_topk(\n",
    "    pipe: Pipeline, X_valid: pd.DataFrame, y_valid: pd.Series,\n",
    "    task: str, topk: int = 20, n_repeats: int = 10\n",
    "):\n",
    "    if task == \"regression\":\n",
    "        scoring = \"neg_mean_absolute_error\"\n",
    "    elif task == \"binary\":\n",
    "        scoring = \"roc_auc\"\n",
    "    else:\n",
    "        scoring = \"neg_log_loss\"\n",
    "\n",
    "    r = permutation_importance(\n",
    "        pipe, X_valid, y_valid,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=RANDOM_STATE,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    feat_names = get_feature_names(pipe)\n",
    "    if not feat_names:\n",
    "        feat_names = [f\"f{i}\" for i in range(len(r.importances_mean))]\n",
    "\n",
    "    imp = pd.DataFrame({\n",
    "        \"feature\": feat_names,\n",
    "        \"importance_mean\": r.importances_mean,\n",
    "        \"importance_std\": r.importances_std,\n",
    "    }).sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "    return imp.head(topk), imp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b26656",
   "metadata": {},
   "source": [
    "## 9. 时间序列特征工程（lag / rolling / 变动率）\n",
    "关键约束：rolling 统计量必须 shift(1)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c90177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_series_features(\n",
    "    df: pd.DataFrame,\n",
    "    group_cols: Optional[List[str]] = None,\n",
    "    time_col: Optional[str] = None,\n",
    "    value_cols: Optional[List[str]] = None,\n",
    "    lags: Tuple[int, ...] = (1, 2, 5, 10),\n",
    "    roll_windows: Tuple[int, ...] = (5, 20, 60),\n",
    "):\n",
    "    out = df.copy()\n",
    "\n",
    "    if time_col is not None:\n",
    "        out = out.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    if value_cols is None:\n",
    "        exclude = set((group_cols or []) + ([time_col] if time_col else []))\n",
    "        value_cols = [c for c in out.columns if c not in exclude and pd.api.types.is_numeric_dtype(out[c])]\n",
    "\n",
    "    if group_cols:\n",
    "        g = out.groupby(group_cols, sort=False)\n",
    "        for c in value_cols:\n",
    "            for L in lags:\n",
    "                out[f\"{c}_lag{L}\"] = g[c].shift(L)\n",
    "            for w in roll_windows:\n",
    "                out[f\"{c}_rollmean{w}\"] = g[c].shift(1).rolling(w).mean()\n",
    "                out[f\"{c}_rollstd{w}\"]  = g[c].shift(1).rolling(w).std()\n",
    "                out[f\"{c}_rollmin{w}\"]  = g[c].shift(1).rolling(w).min()\n",
    "                out[f\"{c}_rollmax{w}\"]  = g[c].shift(1).rolling(w).max()\n",
    "            out[f\"{c}_diff1\"] = g[c].diff(1)\n",
    "            out[f\"{c}_pct1\"]  = g[c].pct_change(1)\n",
    "    else:\n",
    "        for c in value_cols:\n",
    "            for L in lags:\n",
    "                out[f\"{c}_lag{L}\"] = out[c].shift(L)\n",
    "            for w in roll_windows:\n",
    "                out[f\"{c}_rollmean{w}\"] = out[c].shift(1).rolling(w).mean()\n",
    "                out[f\"{c}_rollstd{w}\"]  = out[c].shift(1).rolling(w).std()\n",
    "                out[f\"{c}_rollmin{w}\"]  = out[c].shift(1).rolling(w).min()\n",
    "                out[f\"{c}_rollmax{w}\"]  = out[c].shift(1).rolling(w).max()\n",
    "            out[f\"{c}_diff1\"] = out[c].diff(1)\n",
    "            out[f\"{c}_pct1\"]  = out[c].pct_change(1)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67f662e",
   "metadata": {},
   "source": [
    "## 10. 端到端示例\n",
    "需要替换：DATA_PATH / TARGET / TIME_COL / GROUP_COLS / IS_TIME_SERIES。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668ba8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 改这里 ====\n",
    "DATA_PATH = \"data.csv\"    # 支持 csv/parquet；根据现场实际修改\n",
    "TARGET = \"y\"\n",
    "TIME_COL = None           # 例如 \"date\"\n",
    "GROUP_COLS = None         # 例如 [\"asset_id\"]\n",
    "IS_TIME_SERIES = False    # 时序任务置 True\n",
    "\n",
    "# 读取\n",
    "if DATA_PATH.endswith(\".csv\"):\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "elif DATA_PATH.endswith(\".parquet\"):\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "else:\n",
    "    raise ValueError(\"Unsupported format\")\n",
    "\n",
    "quick_profile(df, target=TARGET, time_col=TIME_COL)\n",
    "\n",
    "# 时间列尝试转 datetime\n",
    "if TIME_COL is not None and TIME_COL in df.columns:\n",
    "    try:\n",
    "        df[TIME_COL] = pd.to_datetime(df[TIME_COL])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "task = infer_task_type(df[TARGET])\n",
    "print(\"inferred task:\", task)\n",
    "\n",
    "# 可选：时序特征工程（按需启用）\n",
    "# df = add_time_series_features(df, group_cols=GROUP_COLS, time_col=TIME_COL)\n",
    "\n",
    "# 删除全空列\n",
    "all_nan_cols = [c for c in df.columns if df[c].isna().all()]\n",
    "if all_nan_cols:\n",
    "    df = df.drop(columns=all_nan_cols)\n",
    "    print(\"dropped all-NaN cols:\", len(all_nan_cols))\n",
    "\n",
    "# 切分\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = split_holdout(\n",
    "    df, target=TARGET,\n",
    "    time_col=TIME_COL if IS_TIME_SERIES else None,\n",
    "    test_size=0.2, valid_size=0.2,\n",
    "    shuffle=not IS_TIME_SERIES,\n",
    "    stratify=(task in (\"binary\", \"multiclass\")) and (not IS_TIME_SERIES)\n",
    ")\n",
    "\n",
    "print(\"splits:\", X_train.shape, X_valid.shape, X_test.shape)\n",
    "\n",
    "# 多模型对比\n",
    "res, pipes = benchmark_models(X_train, y_train, X_valid, y_valid, task, model_names=(\"hgb\",\"rf\",\"et\",\"gbrt\",\"dt\"))\n",
    "display(res.sort_values(res.columns[1], ascending=(task!=\"regression\")))\n",
    "\n",
    "best_name = res.sort_values(res.columns[1], ascending=(task!=\"regression\")).iloc[0][\"model\"]\n",
    "best_pipe = pipes[best_name]\n",
    "print(\"best model:\", best_name)\n",
    "\n",
    "# test 评估\n",
    "yhat_test = best_pipe.predict(X_test)\n",
    "yproba_test = None\n",
    "if task == \"binary\":\n",
    "    yproba_test = best_pipe.predict_proba(X_test)[:, 1]\n",
    "elif task == \"multiclass\":\n",
    "    yproba_test = best_pipe.predict_proba(X_test)\n",
    "\n",
    "test_metrics = evaluate(task, y_test, yhat_test, yproba_test)\n",
    "print(\"test_metrics:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b064e2",
   "metadata": {},
   "source": [
    "## 11. 重要性输出（Top 20）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk, full_imp = permutation_importance_topk(best_pipe, X_valid, y_valid, task, topk=20, n_repeats=8)\n",
    "display(topk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09ce33",
   "metadata": {},
   "source": [
    "## 12. 训练后导出与复用（同机环境）\n",
    "保存一个 pickle，便于复现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ARTIFACT_DIR = \"artifacts\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(ARTIFACT_DIR, f\"pipe_{best_name}_{timestamp()}.pkl\")\n",
    "with open(model_path, \"wb\") as f:\n",
    "    pickle.dump(best_pipe, f)\n",
    "\n",
    "print(\"saved:\", model_path)\n",
    "\n",
    "# 复用：\n",
    "# with open(model_path, \"rb\") as f:\n",
    "#     loaded = pickle.load(f)\n",
    "# loaded.predict(X_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d1b8f",
   "metadata": {},
   "source": [
    "## 13. 面试现场自检清单\n",
    "- 时序任务 shuffle 了\n",
    "- rolling 特征没 shift(1)\n",
    "- 预处理没放 Pipeline（导致统计量泄漏）\n",
    "- 分类只看 accuracy，忽略不均衡（AUC/PR-AUC/F1/LogLoss）\n",
    "- 过拟合不做约束（max_depth / min_samples_leaf / 正则）\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
