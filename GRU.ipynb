{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GRU Dataset Interview 快速部署与训练模板（自用）\n",
        "\n",
        "- 目标：最短时间内跑通 GRU baseline（数据→滑窗→训练→评估→图表→结论要点）。\n",
        "- 关键约束：时间切分防泄漏；标准化只在 train 拟合；滑窗标签严格对齐；全流程可复现。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 使用顺序（按执行）\n",
        "1. 环境与依赖检查\n",
        "2. 读取数据与统一 schema\n",
        "3. 目标构造与缺失处理\n",
        "4. 时间切分\n",
        "5. 滑窗构造\n",
        "6. 标准化\n",
        "7. GRU 训练\n",
        "8. 评估与可视化\n",
        "9. 稳定性与基线\n",
        "10. 导出结果\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) 环境与依赖检查\n",
        "- 记录版本，保证可复现。\n",
        "- 依赖：numpy/pandas/sklearn + torch。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import sys, platform, importlib, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def show_versions():\n",
        "    pkgs = [\"numpy\",\"pandas\",\"sklearn\",\"matplotlib\",\"torch\"]\n",
        "    out = {\"python\": sys.version.split()[0], \"platform\": platform.platform()}\n",
        "    for p in pkgs:\n",
        "        try:\n",
        "            m = importlib.import_module(p)\n",
        "            out[p] = getattr(m, \"__version__\", \"unknown\")\n",
        "        except Exception:\n",
        "            out[p] = None\n",
        "    return out\n",
        "\n",
        "show_versions()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) 读取数据与统一 schema\n",
        "- 规整为：time / (optional id) / target / numeric features。\n",
        "- 排序：单序列按 time；面板按 (id,time)。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "DATA_PATH = \"data.csv\"   # TODO: 现场替换路径\n",
        "time_col = \"timestamp\"   # TODO\n",
        "id_col = None            # 例如 \"asset_id\"；单序列置 None\n",
        "target_col = \"y\"         # TODO\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "assert time_col in df.columns, f\"missing {time_col}\"\n",
        "assert target_col in df.columns, f\"missing {target_col}\"\n",
        "if id_col is not None:\n",
        "    assert id_col in df.columns, f\"missing {id_col}\"\n",
        "\n",
        "df[time_col] = pd.to_datetime(df[time_col])\n",
        "\n",
        "sort_cols = [time_col] if id_col is None else [id_col, time_col]\n",
        "df = df.sort_values(sort_cols).reset_index(drop=True)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 特征列推断\n",
        "- 排除 time/id/target\n",
        "- 仅保留数值型特征；非数值型先记录。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "exclude = {time_col, target_col}\n",
        "if id_col is not None:\n",
        "    exclude.add(id_col)\n",
        "\n",
        "candidate_cols = [c for c in df.columns if c not in exclude]\n",
        "\n",
        "num_cols, cat_cols = [], []\n",
        "for c in candidate_cols:\n",
        "    if pd.api.types.is_numeric_dtype(df[c]):\n",
        "        num_cols.append(c)\n",
        "    else:\n",
        "        cat_cols.append(c)\n",
        "\n",
        "feature_cols = num_cols.copy()\n",
        "\n",
        "print(\"rows:\", len(df))\n",
        "print(\"num_features:\", len(feature_cols))\n",
        "print(\"cat_features:\", len(cat_cols))\n",
        "cat_cols[:10], feature_cols[:10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) 目标构造与缺失处理\n",
        "- horizon 预测：窗口末端为 t，标签取 t+h。\n",
        "- 缺失：ffill/bfill 后仍缺填 0；生成缺失指示特征。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "horizon = 1      # TODO\n",
        "window_L = 64    # TODO\n",
        "\n",
        "for c in feature_cols + [target_col]:\n",
        "    if c in df.columns and pd.api.types.is_numeric_dtype(df[c]):\n",
        "        df[f\"{c}__isna\"] = df[c].isna().astype(np.int8)\n",
        "        if id_col is None:\n",
        "            df[c] = df[c].ffill().bfill()\n",
        "        else:\n",
        "            df[c] = df.groupby(id_col, group_keys=False)[c].apply(lambda s: s.ffill().bfill())\n",
        "        df[c] = df[c].fillna(0.0)\n",
        "\n",
        "feature_cols = feature_cols + [c for c in df.columns if c.endswith(\"__isna\")]\n",
        "\n",
        "df[[time_col] + ([id_col] if id_col else []) + feature_cols[:5] + [target_col]].head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) 时间切分（严格防泄漏）\n",
        "- 按全局时间点切分。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "train_frac, valid_frac = 0.70, 0.15\n",
        "\n",
        "unique_times = np.array(sorted(df[time_col].unique()))\n",
        "nT = len(unique_times)\n",
        "\n",
        "i_train_end = int(np.floor(nT * train_frac))\n",
        "i_valid_end = int(np.floor(nT * (train_frac + valid_frac)))\n",
        "\n",
        "train_time_max = unique_times[i_train_end-1]\n",
        "valid_time_max = unique_times[i_valid_end-1]\n",
        "\n",
        "df[\"split\"] = np.where(df[time_col] <= train_time_max, \"train\",\n",
        "               np.where(df[time_col] <= valid_time_max, \"valid\", \"test\"))\n",
        "\n",
        "df[\"split\"].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) 滑窗构造（N,T,F）\n",
        "- 输出：X (N,L,F), y (N,)\n",
        "- 面板：每个 id 独立滑窗。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from typing import Tuple, Optional, List\n",
        "\n",
        "def make_windows_single(df_part: pd.DataFrame,\n",
        "                        feature_cols: List[str],\n",
        "                        target_col: str,\n",
        "                        time_col: str,\n",
        "                        horizon: int,\n",
        "                        window_L: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    x = df_part[feature_cols].to_numpy(dtype=np.float32)\n",
        "    y = df_part[target_col].to_numpy(dtype=np.float32)\n",
        "    t = df_part[time_col].to_numpy()\n",
        "    split = df_part[\"split\"].to_numpy()\n",
        "\n",
        "    n = len(df_part)\n",
        "    last_end = n - 1 - horizon\n",
        "    if last_end < window_L - 1:\n",
        "        return (np.empty((0, window_L, len(feature_cols)), np.float32),\n",
        "                np.empty((0,), np.float32),\n",
        "                np.empty((0,), object),\n",
        "                np.empty((0,), object))\n",
        "\n",
        "    Xs, ys, ts, ss = [], [], [], []\n",
        "    for end in range(window_L - 1, last_end + 1):\n",
        "        start = end - window_L + 1\n",
        "        Xs.append(x[start:end+1])\n",
        "        ys.append(y[end + horizon])\n",
        "        ts.append(t[end])\n",
        "        ss.append(split[end])\n",
        "    return np.stack(Xs), np.asarray(ys), np.asarray(ss), np.asarray(ts)\n",
        "\n",
        "def make_windows(df: pd.DataFrame,\n",
        "                 feature_cols: List[str],\n",
        "                 target_col: str,\n",
        "                 time_col: str,\n",
        "                 id_col: Optional[str],\n",
        "                 horizon: int,\n",
        "                 window_L: int):\n",
        "    X_all, y_all, split_all, id_all, t_all = [], [], [], [], []\n",
        "    if id_col is None:\n",
        "        X, y, s_end, t_end = make_windows_single(df, feature_cols, target_col, time_col, horizon, window_L)\n",
        "        if len(y):\n",
        "            X_all.append(X); y_all.append(y); split_all.append(s_end)\n",
        "            id_all.append(np.full((len(y),), \"single\", dtype=object))\n",
        "            t_all.append(t_end)\n",
        "    else:\n",
        "        for gid, g in df.groupby(id_col, sort=False):\n",
        "            X, y, s_end, t_end = make_windows_single(g, feature_cols, target_col, time_col, horizon, window_L)\n",
        "            if len(y) == 0:\n",
        "                continue\n",
        "            X_all.append(X); y_all.append(y); split_all.append(s_end)\n",
        "            id_all.append(np.full((len(y),), gid, dtype=object))\n",
        "            t_all.append(t_end)\n",
        "\n",
        "    if not X_all:\n",
        "        raise ValueError(\"no windows generated; check window_L/horizon/data length\")\n",
        "\n",
        "    X = np.concatenate(X_all, axis=0)\n",
        "    y = np.concatenate(y_all, axis=0)\n",
        "    split = np.concatenate(split_all, axis=0)\n",
        "    ids = np.concatenate(id_all, axis=0)\n",
        "    t_end = np.concatenate(t_all, axis=0)\n",
        "    return X, y, split, ids, t_end\n",
        "\n",
        "X, y, split_w, ids_w, t_end_w = make_windows(df, feature_cols, target_col, time_col, id_col, horizon, window_L)\n",
        "X.shape, y.shape, pd.Series(split_w).value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) 标准化（fit train）\n",
        "- 只用 train 窗口拟合 scaler。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "train_mask = (split_w == \"train\")\n",
        "valid_mask = (split_w == \"valid\")\n",
        "test_mask  = (split_w == \"test\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = X[train_mask]\n",
        "Ntr, T, F = X_train.shape\n",
        "scaler.fit(X_train.reshape(Ntr*T, F))\n",
        "\n",
        "def apply_scaler(X_in: np.ndarray) -> np.ndarray:\n",
        "    N, T, F = X_in.shape\n",
        "    return scaler.transform(X_in.reshape(N*T, F)).reshape(N, T, F).astype(np.float32)\n",
        "\n",
        "Xz = apply_scaler(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) GRU 模型与训练\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "# 任务类型\n",
        "task = \"regression\"      # \"regression\" / \"classification\"\n",
        "num_classes = None       # 分类任务时设为 C\n",
        "\n",
        "# 训练超参\n",
        "device = \"cuda\"\n",
        "seed = 42\n",
        "batch_size = 128\n",
        "epochs = 30\n",
        "lr = 1e-3\n",
        "weight_decay = 1e-4\n",
        "grad_clip = 1.0\n",
        "patience = 5\n",
        "\n",
        "# 模型超参\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "dropout = 0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    import random, os\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(seed)\n",
        "device_t = torch.device(\"cuda\" if (device==\"cuda\" and torch.cuda.is_available()) else \"cpu\")\n",
        "device_t\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "class WindowDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.from_numpy(X)\n",
        "        self.y = torch.from_numpy(y)\n",
        "    def __len__(self): return self.X.shape[0]\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "Xtr, ytr = Xz[train_mask], y[train_mask]\n",
        "Xva, yva = Xz[valid_mask], y[valid_mask]\n",
        "Xte, yte = Xz[test_mask],  y[test_mask]\n",
        "\n",
        "if task == \"classification\":\n",
        "    assert num_classes is not None\n",
        "    ytr = ytr.astype(np.int64); yva = yva.astype(np.int64); yte = yte.astype(np.int64)\n",
        "else:\n",
        "    ytr = ytr.astype(np.float32); yva = yva.astype(np.float32); yte = yte.astype(np.float32)\n",
        "\n",
        "dl_tr = DataLoader(WindowDataset(Xtr, ytr), batch_size=batch_size, shuffle=True)\n",
        "dl_va = DataLoader(WindowDataset(Xva, yva), batch_size=batch_size, shuffle=False)\n",
        "dl_te = DataLoader(WindowDataset(Xte, yte), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "(len(dl_tr), len(dl_va), len(dl_te))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout, task, num_classes=None):\n",
        "        super().__init__()\n",
        "        self.task = task\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0\n",
        "        )\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        out_dim = 1 if task == \"regression\" else int(num_classes)\n",
        "        self.head = nn.Linear(hidden_size, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        last = out[:, -1, :]\n",
        "        z = self.drop(last)\n",
        "        return self.head(z)\n",
        "\n",
        "model = GRUModel(F, hidden_size, num_layers, dropout, task, num_classes).to(device_t)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "if task == \"regression\":\n",
        "    loss_fn = nn.SmoothL1Loss()\n",
        "else:\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "def eval_epoch(model, dl):\n",
        "    model.eval()\n",
        "    losses, ys, yhs = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in dl:\n",
        "            xb = xb.to(device_t); yb = yb.to(device_t)\n",
        "            pred = model(xb)\n",
        "            if task == \"regression\":\n",
        "                pred2 = pred.squeeze(-1)\n",
        "                loss = loss_fn(pred2, yb)\n",
        "                yhs.append(pred2.detach().cpu().numpy())\n",
        "                ys.append(yb.detach().cpu().numpy())\n",
        "            else:\n",
        "                loss = loss_fn(pred, yb)\n",
        "                yhs.append(pred.detach().cpu().numpy())\n",
        "                ys.append(yb.detach().cpu().numpy())\n",
        "            losses.append(loss.item())\n",
        "    return float(np.mean(losses)), np.concatenate(ys), np.concatenate(yhs)\n",
        "\n",
        "def train_one_epoch(model, dl):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    for xb, yb in dl:\n",
        "        xb = xb.to(device_t); yb = yb.to(device_t)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        pred = model(xb)\n",
        "        loss = loss_fn(pred.squeeze(-1), yb) if task == \"regression\" else loss_fn(pred, yb)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "    return float(np.mean(losses))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "best_val = float(\"inf\")\n",
        "best_state = None\n",
        "bad = 0\n",
        "history = []\n",
        "\n",
        "for ep in range(1, epochs+1):\n",
        "    tr_loss = train_one_epoch(model, dl_tr)\n",
        "    va_loss, _, _ = eval_epoch(model, dl_va)\n",
        "    history.append({\"epoch\": ep, \"train_loss\": tr_loss, \"val_loss\": va_loss})\n",
        "    print(f\"epoch {ep:03d} | train {tr_loss:.6f} | val {va_loss:.6f}\")\n",
        "\n",
        "    if va_loss + 1e-8 < best_val:\n",
        "        best_val = va_loss\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "        bad = 0\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            print(\"early stop\")\n",
        "            break\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "best_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) 评估与可视化\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "te_loss, y_te, yh_te = eval_epoch(model, dl_te)\n",
        "\n",
        "if task == \"regression\":\n",
        "    yhat = yh_te.reshape(-1)\n",
        "    rmse = float(np.sqrt(mean_squared_error(y_te, yhat)))\n",
        "    mae  = float(mean_absolute_error(y_te, yhat))\n",
        "    r2   = float(r2_score(y_te, yhat))\n",
        "    metrics = {\"test_loss\": float(te_loss), \"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
        "    metrics\n",
        "else:\n",
        "    logits = yh_te\n",
        "    pred = logits.argmax(axis=1)\n",
        "    acc = float(accuracy_score(y_te, pred))\n",
        "    cm = confusion_matrix(y_te, pred)\n",
        "    metrics = {\"test_loss\": float(te_loss), \"accuracy\": acc}\n",
        "    metrics, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "if task == \"regression\":\n",
        "    yhat = yh_te.reshape(-1)\n",
        "    plt.figure()\n",
        "    plt.scatter(y_te, yhat, s=6)\n",
        "    plt.xlabel(\"y_true\"); plt.ylabel(\"y_pred\")\n",
        "    plt.title(\"Test: Pred vs True\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) 稳定性检查与对照基线\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "def bucket_by_time(t_arr, n_buckets=5):\n",
        "    order = np.argsort(t_arr)\n",
        "    return np.array_split(order, n_buckets)\n",
        "\n",
        "if task == \"regression\":\n",
        "    test_idx = np.where(test_mask)[0]\n",
        "    buckets = bucket_by_time(t_end_w[test_idx], n_buckets=5)\n",
        "    rows = []\n",
        "    for bi, b in enumerate(buckets):\n",
        "        idx = test_idx[b]\n",
        "        dl_b = DataLoader(WindowDataset(Xz[idx], y[idx].astype(np.float32)), batch_size=512, shuffle=False)\n",
        "        _, y_b, yh_b = eval_epoch(model, dl_b)\n",
        "        yhat = yh_b.reshape(-1)\n",
        "        rows.append({\n",
        "            \"bucket\": bi,\n",
        "            \"n\": int(len(idx)),\n",
        "            \"rmse\": float(np.sqrt(mean_squared_error(y_b, yhat))),\n",
        "            \"mae\": float(mean_absolute_error(y_b, yhat)),\n",
        "        })\n",
        "    display(pd.DataFrame(rows))\n",
        "\n",
        "# baseline：Ridge(last-step)\n",
        "X_last = Xz[:, -1, :]\n",
        "Xtr_b, Xte_b = X_last[train_mask], X_last[test_mask]\n",
        "ytr_b, yte_b = y[train_mask], y[test_mask]\n",
        "if task == \"regression\":\n",
        "    ridge = Ridge(alpha=1.0)\n",
        "    ridge.fit(Xtr_b, ytr_b)\n",
        "    pred = ridge.predict(Xte_b)\n",
        "    ridge_rmse = float(np.sqrt(mean_squared_error(yte_b, pred)))\n",
        "    {\"ridge_rmse\": ridge_rmse, \"gru_rmse\": metrics.get(\"rmse\")}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) 导出结果（用于 PPT）\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "import os, json\n",
        "out_dir = \"outputs_gru\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "hist_df = pd.DataFrame(history)\n",
        "hist_df.to_csv(os.path.join(out_dir, \"train_history.csv\"), index=False)\n",
        "\n",
        "with open(os.path.join(out_dir, \"metrics.json\"), \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "hist_df.tail(), metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "def ppt_bullets(metrics: dict, task: str):\n",
        "    if task == \"regression\":\n",
        "        return [\n",
        "            f\"GRU test RMSE={metrics['rmse']:.4f}, MAE={metrics['mae']:.4f}, R2={metrics['r2']:.4f}\",\n",
        "            \"时间切分完成；标准化仅在 train 拟合；滑窗标签对齐已验证\",\n",
        "            f\"窗口 L={window_L}, horizon h={horizon}, hidden={hidden_size}, layers={num_layers}, dropout={dropout}\",\n",
        "            \"稳定性：按时间分桶输出分段 RMSE/MAE（见 notebook）\",\n",
        "            \"对照：Ridge(last-step) baseline 已跑，用于校验增益与避免泄漏误判\",\n",
        "        ]\n",
        "    else:\n",
        "        return [\n",
        "            f\"GRU test accuracy={metrics['accuracy']:.4f}\",\n",
        "            \"时间切分完成；标准化仅在 train 拟合；滑窗标签对齐已验证\",\n",
        "            f\"窗口 L={window_L}, horizon h={horizon}, hidden={hidden_size}, layers={num_layers}, dropout={dropout}\",\n",
        "            \"稳定性：按时间分桶/按实体分组输出（见 notebook）\",\n",
        "            \"对照：baseline 已跑，用于校验增益与避免泄漏误判\",\n",
        "        ]\n",
        "\n",
        "ppt_bullets(metrics, task)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}