{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 脏数据清洗速查本（Dataset Interview 用）\n\n这份 notebook 用来在现场快速落地：数据体检 → 切分与泄露防控 → 清洗（缺失/异常/重复/类型/类别脏值）→ 产出可复现的 Pipeline → A/B 对比。\n\n**目标**：\n- 先拿到一个能跑通的 baseline（最小可用清洗）\n- 再集中火力处理最大的噪声源（高缺失、明显错误、强异常、泄露风险）\n- 每一步保留日志：改了多少行、改了多少值、分布变化如何\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. 运行前约定\n- 原始数据只读：`df_raw` 永远保留不改\n- 每一步清洗产出新对象：`df = df.copy()`\n- 任何用到统计量（均值/标准差/分位数/编码等）的步骤：只在 train 上 fit，在 test 上 transform\n- 时间序列：先锁定时间切分，再做任何特征工程\n- 产出物：\n  - `quality_report.csv`（列级质量表）\n  - `cleaning_log.json`（清洗日志）\n  - `pipeline.pkl`（可复现的 sklearn Pipeline，如果需要）\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\n\nfrom dataclasses import dataclass, asdict\nimport json\nfrom pathlib import Path\n\n# 仅用 sklearn / pandas / numpy 的通用做法\nfrom sklearn.model_selection import TimeSeriesSplit, GroupKFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import Ridge, Lasso, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "OUTPUT_DIR = Path(\"outputs\")\nOUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n\ncleaning_log = []\n\ndef log_step(name: str, details: dict):\n    rec = {\"step\": name, **details}\n    cleaning_log.append(rec)\n\ndef save_log(path=OUTPUT_DIR/\"cleaning_log.json\"):\n    path.write_text(json.dumps(cleaning_log, ensure_ascii=False, indent=2))\n    return path\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. 数据加载\n把下面的读取代码替换成现场数据路径与格式。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 示例：CSV\n# df_raw = pd.read_csv(\"data.csv\")\n\n# 示例：Parquet\n# df_raw = pd.read_parquet(\"data.parquet\")\n\n# 本 cell 运行后确保有 df_raw\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. 快速体检（Profiling）\n输出列级质量表：缺失率、唯一值数、疑似常数列、数值分位数、类别 top-k、时间列单调性等。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def infer_column_roles(df: pd.DataFrame, time_hint=None, target_hint=None, id_hints=None):\n    \"\"\"粗略推断列角色：time / target / id / numeric / categorical.\"\"\"\n    cols = list(df.columns)\n    time_col = time_hint if time_hint in cols else None\n    target_col = target_hint if target_hint in cols else None\n    id_cols = [c for c in (id_hints or []) if c in cols]\n\n    # 初步：object + category 视为 categorical，数值为 numeric\n    numeric_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df[c])]\n    categorical_cols = [c for c in cols if (c not in numeric_cols)]\n    # 排除已知角色\n    for c in [time_col, target_col, *id_cols]:\n        if c in numeric_cols: numeric_cols.remove(c)\n        if c in categorical_cols: categorical_cols.remove(c)\n\n    return {\n        \"time_col\": time_col,\n        \"target_col\": target_col,\n        \"id_cols\": id_cols,\n        \"numeric_cols\": numeric_cols,\n        \"categorical_cols\": categorical_cols,\n    }\n\ndef quality_report(df: pd.DataFrame, topk=5) -> pd.DataFrame:\n    rows = []\n    n = len(df)\n    for c in df.columns:\n        s = df[c]\n        missing = s.isna().mean()\n        nunique = s.nunique(dropna=True)\n        is_const = (nunique <= 1)\n        dtype = str(s.dtype)\n\n        row = {\n            \"col\": c,\n            \"dtype\": dtype,\n            \"missing_rate\": float(missing),\n            \"nunique\": int(nunique),\n            \"is_constant\": bool(is_const),\n        }\n\n        if pd.api.types.is_numeric_dtype(s):\n            q = s.quantile([0.0, 0.01, 0.05, 0.5, 0.95, 0.99, 1.0], interpolation=\"linear\")\n            row.update({f\"q{int(k*100):02d}\": float(v) if pd.notna(v) else np.nan for k, v in q.items()})\n        else:\n            vc = s.astype(\"object\").value_counts(dropna=True).head(topk)\n            row.update({f\"top{i+1}\": str(k) for i, k in enumerate(vc.index)})\n            row.update({f\"top{i+1}_cnt\": int(v) for i, v in enumerate(vc.values)})\n\n        rows.append(row)\n\n    rep = pd.DataFrame(rows).sort_values([\"missing_rate\", \"nunique\"], ascending=[False, True])\n    return rep\n\ndef monotonicity_check(df: pd.DataFrame, time_col: str):\n    s = df[time_col]\n    if not pd.api.types.is_datetime64_any_dtype(s):\n        s2 = pd.to_datetime(s, errors=\"coerce\")\n    else:\n        s2 = s\n    valid = s2.dropna()\n    if len(valid) <= 2:\n        return {\"is_monotonic_increasing\": None, \"duplicate_rate\": None}\n    return {\n        \"is_monotonic_increasing\": bool(valid.is_monotonic_increasing),\n        \"duplicate_rate\": float(valid.duplicated().mean()),\n        \"nat_rate\": float(s2.isna().mean()),\n    }\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 需要在现场填的参数 ===\nTIME_COL = None      # e.g. \"timestamp\"\nTARGET_COL = None    # e.g. \"y\"\nID_COLS = []         # e.g. [\"asset_id\", \"instrument\", \"user_id\"]\n\nroles = infer_column_roles(df_raw, time_hint=TIME_COL, target_hint=TARGET_COL, id_hints=ID_COLS)\nroles\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "rep = quality_report(df_raw)\nrep.head(30)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "rep_path = OUTPUT_DIR / \"quality_report.csv\"\nrep.to_csv(rep_path, index=False)\nlog_step(\"quality_report\", {\"path\": str(rep_path), \"n_rows\": int(len(df_raw)), \"n_cols\": int(df_raw.shape[1])})\nrep_path\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 2.1 快速排雷清单\n- 主键是否唯一（如 `time + id`）\n- 时间列是否能解析、是否乱序、是否重复\n- target 是否存在明显泄露（例如特征里出现 target 变体、或者未来信息）\n- 是否存在超高缺失/常数列（直接丢弃）\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def check_key_uniqueness(df: pd.DataFrame, key_cols):\n    if not key_cols:\n        return {\"key_cols\": key_cols, \"is_unique\": None, \"dup_rate\": None}\n    dup_rate = df.duplicated(subset=key_cols, keep=False).mean()\n    return {\"key_cols\": key_cols, \"is_unique\": float(dup_rate) == 0.0, \"dup_rate\": float(dup_rate)}\n\n# e.g. KEY_COLS = [TIME_COL] + ID_COLS (如果这些应当唯一)\nKEY_COLS = [c for c in ([TIME_COL] + ID_COLS) if c]\nkey_check = check_key_uniqueness(df_raw, KEY_COLS)\nlog_step(\"key_uniqueness_check\", key_check)\nkey_check\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if TIME_COL:\n    mono = monotonicity_check(df_raw, TIME_COL)\n    log_step(\"monotonicity_check\", {\"time_col\": TIME_COL, **mono})\n    mono\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. 切分（先锁边界再清洗）\n时间序列默认按时间切分：train 在前，test 在后。\n\n如果存在实体（如 asset/instrument/user），可选：\n- 训练/验证按时间滚动（TimeSeriesSplit）\n- 或按 group 切分（GroupKFold）防止同一实体泄露\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def time_train_test_split(df: pd.DataFrame, time_col: str, test_size=0.2):\n    d = df.copy()\n    d[time_col] = pd.to_datetime(d[time_col], errors=\"coerce\")\n    d = d.sort_values(time_col)\n    n = len(d)\n    cut = int(np.floor(n * (1 - test_size)))\n    train = d.iloc[:cut].copy()\n    test = d.iloc[cut:].copy()\n    return train, test\n\n# 用法：\n# train_df, test_df = time_train_test_split(df_raw, TIME_COL, test_size=0.2)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. 最小可用清洗（先跑通 baseline）\n步骤：\n- 解析时间（如果有）\n- 去掉常数列 / 极高缺失列\n- 处理明显类型错误（数值列里混了字符串）\n- 主键重复：聚合或取最新\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def drop_constant_and_high_missing(df: pd.DataFrame, missing_thresh=0.95):\n    before_cols = df.shape[1]\n    miss = df.isna().mean()\n    high_miss_cols = miss[miss >= missing_thresh].index.tolist()\n    nunique = df.nunique(dropna=True)\n    const_cols = nunique[nunique <= 1].index.tolist()\n\n    drop_cols = sorted(set(high_miss_cols) | set(const_cols))\n    out = df.drop(columns=drop_cols, errors=\"ignore\")\n\n    log_step(\"drop_constant_high_missing\", {\n        \"missing_thresh\": float(missing_thresh),\n        \"dropped_cols\": drop_cols,\n        \"n_dropped\": int(len(drop_cols)),\n        \"before_cols\": int(before_cols),\n        \"after_cols\": int(out.shape[1]),\n    })\n    return out\n\ndef coerce_numeric_like_columns(df: pd.DataFrame, cols):\n    out = df.copy()\n    changed = {}\n    for c in cols:\n        if c not in out.columns:\n            continue\n        if pd.api.types.is_numeric_dtype(out[c]):\n            continue\n        # 尝试把 object -> numeric\n        s0 = out[c]\n        s1 = pd.to_numeric(s0.astype(\"string\").str.replace(\",\", \"\", regex=False)\n                                 .str.replace(\"%\", \"\", regex=False)\n                                 .str.strip(), errors=\"coerce\")\n        # 记录变化：新产生的 NaN 数、有效数\n        changed[c] = {\n            \"before_nonnull\": int(s0.notna().sum()),\n            \"after_nonnull\": int(s1.notna().sum()),\n            \"new_nan\": int((s1.isna() & s0.notna()).sum()),\n        }\n        out[c] = s1\n    log_step(\"coerce_numeric_like\", {\"cols\": cols, \"changed\": changed})\n    return out\n\ndef deduplicate_by_key(df: pd.DataFrame, key_cols, keep=\"last\", sort_col=None):\n    out = df.copy()\n    if sort_col:\n        out = out.sort_values(sort_col)\n    before = len(out)\n    out = out.drop_duplicates(subset=key_cols, keep=keep)\n    after = len(out)\n    log_step(\"deduplicate\", {\"key_cols\": key_cols, \"keep\": keep, \"sort_col\": sort_col, \"before\": int(before), \"after\": int(after)})\n    return out\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 示例流程（按现场数据修改） ===\ndf = df_raw.copy()\n\n# 时间解析 + 排序（如果有）\nif TIME_COL and TIME_COL in df.columns:\n    df[TIME_COL] = pd.to_datetime(df[TIME_COL], errors=\"coerce\")\n    df = df.sort_values(TIME_COL)\n\n# 删除常数列 / 高缺失列\ndf = drop_constant_and_high_missing(df, missing_thresh=0.98)\n\n# 如果某些列应该是数值但读成了 object，在这里列出\nNUMERIC_LIKE = []  # e.g. [\"price\", \"volume\"]\ndf = coerce_numeric_like_columns(df, NUMERIC_LIKE)\n\n# 主键去重（如果需要）\nif KEY_COLS:\n    sort_col = TIME_COL if TIME_COL else None\n    df = deduplicate_by_key(df, KEY_COLS, keep=\"last\", sort_col=sort_col)\n\ndf.shape\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. 异常值处理（不盲删）\n两类异常：\n- 物理/业务不可能：负价格、负成交量、极端时间戳 → 置为缺失 or 修正\n- 极端但可能真实：厚尾分布 → clip/winsorize/log1p/robust scaler\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def clip_by_quantile(df: pd.DataFrame, cols, q_low=0.01, q_high=0.99):\n    out = df.copy()\n    bounds = {}\n    for c in cols:\n        if c not in out.columns: \n            continue\n        if not pd.api.types.is_numeric_dtype(out[c]):\n            continue\n        lo = out[c].quantile(q_low)\n        hi = out[c].quantile(q_high)\n        bounds[c] = {\"lo\": float(lo) if pd.notna(lo) else np.nan,\n                     \"hi\": float(hi) if pd.notna(hi) else np.nan}\n        out[c] = out[c].clip(lower=lo, upper=hi)\n    log_step(\"clip_by_quantile\", {\"cols\": cols, \"q_low\": float(q_low), \"q_high\": float(q_high), \"bounds\": bounds})\n    return out\n\ndef set_impossible_to_nan(df: pd.DataFrame, rules: dict):\n    \"\"\"rules: {col: ('>=', 0) / ('between', (a,b)) / ... }\"\"\"\n    out = df.copy()\n    changed = {}\n    for c, rule in rules.items():\n        if c not in out.columns: \n            continue\n        op = rule[0]\n        before_nan = int(out[c].isna().sum())\n        if op == \">=\":\n            thr = rule[1]\n            mask = out[c].notna() & (out[c] < thr)\n            out.loc[mask, c] = np.nan\n        elif op == \"<=\":\n            thr = rule[1]\n            mask = out[c].notna() & (out[c] > thr)\n            out.loc[mask, c] = np.nan\n        elif op == \"between\":\n            a, b = rule[1]\n            mask = out[c].notna() & ~out[c].between(a, b)\n            out.loc[mask, c] = np.nan\n        else:\n            raise ValueError(f\"Unsupported op: {op}\")\n        after_nan = int(out[c].isna().sum())\n        changed[c] = {\"nan_before\": before_nan, \"nan_after\": after_nan, \"new_nan\": after_nan - before_nan}\n    log_step(\"set_impossible_to_nan\", {\"rules\": rules, \"changed\": changed})\n    return out\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 例：明显不可能值置为缺失（按现场业务定义）\nIMPOSSIBLE_RULES = {\n    # \"price\": (\">=\", 0),\n    # \"volume\": (\">=\", 0),\n}\ndf = set_impossible_to_nan(df, IMPOSSIBLE_RULES)\n\n# 例：对数值列做分位数截断（厚尾稳健）\nCLIP_COLS = []  # e.g. [\"price\", \"volume\", \"ret\"]\ndf = clip_by_quantile(df, CLIP_COLS, q_low=0.005, q_high=0.995)\n\ndf.shape\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. 类别清洗（大小写/空格/长尾合并）\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def clean_text_categories(df: pd.DataFrame, cols):\n    out = df.copy()\n    changed = {}\n    for c in cols:\n        if c not in out.columns:\n            continue\n        s0 = out[c]\n        s1 = s0.astype(\"string\").str.strip().str.lower()\n        out[c] = s1\n        changed[c] = {\"nunique_before\": int(s0.nunique(dropna=True)),\n                      \"nunique_after\": int(s1.nunique(dropna=True))}\n    log_step(\"clean_text_categories\", {\"cols\": cols, \"changed\": changed})\n    return out\n\ndef group_rare_categories(df: pd.DataFrame, col: str, min_freq=50, other_label=\"__other__\"):\n    out = df.copy()\n    vc = out[col].value_counts(dropna=False)\n    rare = vc[vc < min_freq].index\n    out[col] = out[col].where(~out[col].isin(rare), other_label)\n    log_step(\"group_rare_categories\", {\"col\": col, \"min_freq\": int(min_freq), \"n_rare\": int(len(rare))})\n    return out\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "CAT_COLS = roles.get(\"categorical_cols\", [])\n# 如果存在明显的类别列（非时间/非目标/非 ID），先做清洗\ndf = clean_text_categories(df, CAT_COLS)\n\n# 对某些高基数类别列做长尾合并（按现场需要挑选）\n# df = group_rare_categories(df, \"category_col\", min_freq=100)\n\ndf.shape\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. 时序特征工程（只用过去信息）\n常见：lag/rolling/ewm、按实体 group 做统计。所有滚动计算用 `shift(1)` 防止把当前时刻信息泄露进特征。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def add_lag_features(df: pd.DataFrame, group_cols, time_col, value_cols, lags=(1, 2, 5)):\n    out = df.copy()\n    out = out.sort_values([*group_cols, time_col]) if group_cols else out.sort_values(time_col)\n    for c in value_cols:\n        for k in lags:\n            out[f\"{c}_lag{k}\"] = out.groupby(group_cols)[c].shift(k) if group_cols else out[c].shift(k)\n    log_step(\"add_lag_features\", {\"group_cols\": group_cols, \"time_col\": time_col, \"value_cols\": value_cols, \"lags\": list(lags)})\n    return out\n\ndef add_rolling_features(df: pd.DataFrame, group_cols, time_col, value_cols, windows=(5, 20), funcs=(\"mean\", \"std\")):\n    out = df.copy()\n    out = out.sort_values([*group_cols, time_col]) if group_cols else out.sort_values(time_col)\n    for c in value_cols:\n        base = out.groupby(group_cols)[c] if group_cols else out[c]\n        shifted = base.shift(1)  # 关键：只用过去\n        for w in windows:\n            roll = shifted.rolling(window=w, min_periods=max(2, w//3))\n            if \"mean\" in funcs:\n                out[f\"{c}_roll{w}_mean\"] = roll.mean()\n            if \"std\" in funcs:\n                out[f\"{c}_roll{w}_std\"] = roll.std()\n    log_step(\"add_rolling_features\", {\"group_cols\": group_cols, \"time_col\": time_col, \"value_cols\": value_cols, \"windows\": list(windows), \"funcs\": list(funcs)})\n    return out\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 现场选择：哪一些数值列值得做 lag/rolling（通常是价格、收益、成交量等）\nTS_VALUE_COLS = []  # e.g. [\"price\", \"ret\", \"volume\"]\nGROUP_COLS = [c for c in ID_COLS if c]  # e.g. [\"asset_id\"]\n\nif TIME_COL and TS_VALUE_COLS:\n    df = add_lag_features(df, group_cols=GROUP_COLS, time_col=TIME_COL, value_cols=TS_VALUE_COLS, lags=(1,2,5))\n    df = add_rolling_features(df, group_cols=GROUP_COLS, time_col=TIME_COL, value_cols=TS_VALUE_COLS, windows=(5,20), funcs=(\"mean\",\"std\"))\n\ndf.shape\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. 泄露排查（快速版）\n常见泄露来源：\n- 特征包含未来值（next/future/lead）\n- 特征直接等于 target 或 target 的线性变换\n- 目标编码/标准化/分位数等在全样本 fit\n- 用到了同一时刻的聚合但没有 shift\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import re\n\ndef suspicious_feature_names(cols):\n    patt = re.compile(r\"(future|next|lead|target|label|y\\b)\", re.IGNORECASE)\n    return [c for c in cols if patt.search(c)]\n\ndef target_correlation_scan(df: pd.DataFrame, target_col: str, topk=20):\n    if target_col not in df.columns:\n        return None\n    y = df[target_col]\n    num_cols = [c for c in df.columns if c != target_col and pd.api.types.is_numeric_dtype(df[c])]\n    corr = {}\n    for c in num_cols:\n        s = df[c]\n        m = y.notna() & s.notna()\n        if m.sum() < 50:\n            continue\n        corr[c] = float(np.corrcoef(y[m], s[m])[0,1])\n    out = pd.Series(corr).dropna().sort_values(key=lambda x: x.abs(), ascending=False).head(topk)\n    return out\n\nsus = suspicious_feature_names(df.columns)\nlog_step(\"suspicious_feature_names\", {\"matches\": sus})\nsus[:30]\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if TARGET_COL:\n    corr_top = target_correlation_scan(df, TARGET_COL, topk=30)\n    corr_top\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. 建模用 Pipeline（可复现、避免泄露）\n数值：median 填补 + RobustScaler\n类别：most_frequent 填补 + OneHotEncoder(handle_unknown='ignore')\n\n模型：\n- 回归：Ridge / RandomForestRegressor\n- 分类：LogisticRegression / RandomForestClassifier\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def build_pipeline(numeric_cols, categorical_cols, task=\"regression\", model_name=\"ridge\"):\n    numeric_tf = Pipeline(steps=[\n        (\"imputer\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n        (\"scaler\", RobustScaler(with_centering=True)),\n    ])\n    categorical_tf = Pipeline(steps=[\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)),\n    ])\n\n    pre = ColumnTransformer(\n        transformers=[\n            (\"num\", numeric_tf, numeric_cols),\n            (\"cat\", categorical_tf, categorical_cols),\n        ],\n        remainder=\"drop\",\n        sparse_threshold=0.3,\n    )\n\n    if task == \"regression\":\n        if model_name == \"ridge\":\n            model = Ridge(alpha=1.0, random_state=0)\n        elif model_name == \"rf\":\n            model = RandomForestRegressor(n_estimators=400, random_state=0, n_jobs=-1)\n        else:\n            raise ValueError(\"unknown model\")\n    else:\n        if model_name == \"logreg\":\n            model = LogisticRegression(max_iter=2000)\n        elif model_name == \"rf\":\n            model = RandomForestClassifier(n_estimators=400, random_state=0, n_jobs=-1)\n        else:\n            raise ValueError(\"unknown model\")\n\n    pipe = Pipeline(steps=[(\"pre\", pre), (\"model\", model)])\n    return pipe\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 训练/测试构造 ===\ndf_model = df.copy()\n\n# 丢掉时间列（一般不直接喂入原始 timestamp）\ndrop_cols = [c for c in [TIME_COL] if c]\ndf_model = df_model.drop(columns=drop_cols, errors=\"ignore\")\n\nassert TARGET_COL is None or TARGET_COL in df_raw.columns or TARGET_COL in df_model.columns, \"TARGET_COL 未设置或不存在\"\n\ndf_model.shape\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# === 简单 time split（如果 TIME_COL 有设置，则用 df_raw 的排序切分更稳） ===\nif TIME_COL and TARGET_COL:\n    train_df, test_df = time_train_test_split(df, TIME_COL, test_size=0.2)\n    train_df = train_df.drop(columns=[TIME_COL], errors=\"ignore\")\n    test_df  = test_df.drop(columns=[TIME_COL], errors=\"ignore\")\nelse:\n    # 兜底：随机切分在时序任务里可能不合适，只用于跑通\n    from sklearn.model_selection import train_test_split\n    train_df, test_df = train_test_split(df_model, test_size=0.2, random_state=0)\n\nif TARGET_COL:\n    X_train = train_df.drop(columns=[TARGET_COL], errors=\"ignore\")\n    y_train = train_df[TARGET_COL]\n    X_test = test_df.drop(columns=[TARGET_COL], errors=\"ignore\")\n    y_test = test_df[TARGET_COL]\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if TARGET_COL:\n    # 重新识别数值/类别列（基于训练集）\n    numeric_cols = [c for c in X_train.columns if pd.api.types.is_numeric_dtype(X_train[c])]\n    categorical_cols = [c for c in X_train.columns if c not in numeric_cols]\n\n    pipe = build_pipeline(numeric_cols, categorical_cols, task=\"regression\", model_name=\"ridge\")\n    pipe\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "if TARGET_COL:\n    pipe.fit(X_train, y_train)\n    pred = pipe.predict(X_test)\n\n    rmse = float(np.sqrt(mean_squared_error(y_test, pred)))\n    mae = float(mean_absolute_error(y_test, pred))\n    r2 = float(r2_score(y_test, pred))\n\n    metrics = {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n    log_step(\"baseline_metrics\", metrics)\n    metrics\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 10. A/B 对比记录\n同一套切分下，比较：\n- baseline（最小清洗）\n- + clip / + rare merge / + rolling / + 更强模型\n\n只要保证每次改动都写进 log，就能快速解释每个步骤带来的收益/代价。\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "ab_results = []\n\ndef record_result(name: str, metrics: dict, notes: str = \"\"):\n    rec = {\"name\": name, **metrics, \"notes\": notes}\n    ab_results.append(rec)\n    log_step(\"ab_record\", rec)\n\n# 用法：\n# record_result(\"baseline_ridge\", metrics, notes=\"median+indicator, robust scaler, ohe\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 11. 导出\n- 日志：`cleaning_log.json`\n- 清洗后数据：`df_clean.parquet`（如允许）\n- A/B 结果：`ab_results.csv`\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 保存日志\nlog_path = save_log()\nlog_path\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 保存 A/B\nif len(ab_results) > 0:\n    ab_path = OUTPUT_DIR / \"ab_results.csv\"\n    pd.DataFrame(ab_results).to_csv(ab_path, index=False)\n    ab_path\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# 保存清洗后的数据（可按需开启）\n# clean_path = OUTPUT_DIR / \"df_clean.parquet\"\n# df.to_parquet(clean_path, index=False)\n# clean_path\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 12. 面试口述模板（写在这里方便照着讲）\n- 先做列级质量表，确认缺失、常数列、类型错、主键重复、时间乱序。\n- 再锁定切分方式（时间切分 / group 切分），避免任何泄露。\n- 先做最小可用清洗跑通 baseline。\n- 针对最大噪声源做专项处理：不可能值→缺失，厚尾→clip，类别脏值→规范化/长尾合并，时序特征→shift(1)+rolling。\n- 每一步都有日志与 A/B，对比收益与风险。\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "name": "Dirty_Data_Cleaning_Playbook.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}