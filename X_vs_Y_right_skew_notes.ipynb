{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# X 右偏 vs Y 右偏：面试速查笔记（Dataset Interview）\n",
        "\n",
        "*更新时间：2026-01-28*\n",
        "\n",
        "目标：区分 **特征分布右偏（X right-skew）** 与 **目标/标签右偏（Y right-skew）** 的含义、风险点、处理方式，以及面试时的表达框架。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 0. 一句话结论\n",
        "\n",
        "- **X 右偏**：主要是 *特征尺度/表示* 问题，影响距离度量、正则化权重、数值稳定性与可解释性；对树模型相对不敏感。\n",
        "- **Y 右偏**：主要是 *损失函数与训练目标* 问题，会让 **MSE/RMSE 被尾部样本主导**，常伴随异方差与残差偏态；对任何模型都可能造成训练目标偏移。\n",
        "\n",
        "优先级：通常 **先处理 Y，再处理 X（如果需要）**。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. 右偏（right-skew）是什么\n",
        "\n",
        "右偏：样本大量集中在较小值处，少数样本特别大，形成长右尾。\n",
        "\n",
        "常见例子：金额/成交量/等待时间/计数类/波动率相关量等。\n",
        "\n",
        "关键词：**长尾、极端值、异方差、重尾残差、尺度主导**。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. X 右偏（feature right-skew）\n",
        "\n",
        "### 2.1 影响对象\n",
        "- 影响“模型如何利用特征”：内积、距离、梯度尺度、正则化实际强度。\n",
        "\n",
        "### 2.2 不处理的典型后果\n",
        "1) **距离/内积被大值支配**：KNN/SVM(RBF)/KMeans/PCA/线性模型等更明显。\n",
        "2) **正则化偏置**：同样的 L1/L2 惩罚下，大尺度特征更容易在模型中留下“痕迹”，与真实重要性未必一致。\n",
        "3) **优化不稳定**：梯度幅度差异大，训练对学习率与初始化更敏感。\n",
        "\n",
        "### 2.3 对模型的敏感度（经验）\n",
        "- 更敏感：线性回归/岭回归/Lasso、SVM、KNN、KMeans、PCA、神经网络（未良好归一化时）。\n",
        "- 相对不敏感：树模型（RF/GBDT），因为分裂按排序阈值，不直接用欧氏距离；但极端值仍可能影响分裂点与叶子统计。\n",
        "\n",
        "### 2.4 常用处理动作（X）\n",
        "- `log1p(X)`：适用于非负特征（含 0）。\n",
        "- **Yeo-Johnson**：可含负数/0，参数 λ 自动拟合。\n",
        "- `RobustScaler` / `QuantileTransformer`：针对重尾、离群点。\n",
        "- `clip/winsorize`：限制极端值的影响（金融里常见）。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Y 右偏（target right-skew）\n",
        "\n",
        "### 3.1 影响对象\n",
        "- 直接影响损失函数的含义与训练目标：在 **原尺度** 上做 MSE/RMSE 时，少数大 y 会主导训练。\n",
        "\n",
        "### 3.2 不处理的典型后果\n",
        "1) **MSE/RMSE 被尾部主导**：模型把主要容量用在拟合极端大值，牺牲主体区间。\n",
        "2) **异方差更常见**：y 越大方差越大，残差对 y 有结构性依赖。\n",
        "3) **残差偏态/重尾**：线性模型的经典推断假设（近似正态、同方差）更易失效。\n",
        "\n",
        "### 3.3 常用处理动作（Y）\n",
        "- `log1p(y)`：y ≥ 0 时最常用，把“绝对误差”更接近地转成“相对误差/倍数误差”。\n",
        "- **Yeo-Johnson(y)**：y 可为负时使用（如收益、价差）。\n",
        "- 换 loss：`MAE` / `Huber` / `Quantile`（更稳健，不让尾部完全支配）。\n",
        "\n",
        "### 3.4 反变换（inverse transform）注意点\n",
        "- 训练在变换空间，最终预测需回到原尺度：\n",
        "  - log1p：`y_hat = expm1(z_hat)`\n",
        "  - YJ：`inverse_transform`\n",
        "- 反变换存在 **Jensen 偏差**：`E[exp(Z)] != exp(E[Z])`，回原尺度后可能对大值系统性偏差。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. X 右偏 vs Y 右偏：面试对照表\n",
        "\n",
        "| 维度 | X 右偏 | Y 右偏 |\n",
        "|---|---|---|\n",
        "| 主要影响 | 特征表示/尺度/距离/正则化 | 损失函数含义/训练目标 |\n",
        "| 对 MSE 合理性的冲击 | 间接 | **直接**（尾部主导） |\n",
        "| 是否常伴随异方差 | 可能 | **非常常见** |\n",
        "| 树模型敏感度 | 低-中 | 中-高（取决于目标/评价指标） |\n",
        "| 常用处理 | scale/log/YJ/robust/clip | log1p/YJ/换 loss |\n",
        "| 优先级 | 视模型与管线而定 | 通常更高 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. 快速诊断清单（EDA 时顺手做）\n",
        "\n",
        "### 5.1 看分布\n",
        "- `hist + log-scale`（x 轴或 y 轴取对数）\n",
        "- 分位数：P50/P90/P99/max\n",
        "\n",
        "### 5.2 看异方差\n",
        "- `residual vs fitted`：残差随预测值增大而扩散 ⇒ 异方差\n",
        "- `abs(residual) vs fitted`：更直观\n",
        "\n",
        "### 5.3 看尾部是否主导指标\n",
        "- 对比 MAE vs RMSE：RMSE 明显更大且不稳定 ⇒ 尾部影响强\n",
        "- 分桶误差（按 y 分位数）：尾部桶的误差贡献占比是否过高\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 一些轻量级工具函数：分位数摘要 & 偏度\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def quantile_summary(a, qs=(0.5, 0.9, 0.95, 0.99)):\n",
        "    a = np.asarray(a)\n",
        "    a = a[np.isfinite(a)]\n",
        "    out = {f\"p{int(q*100)}\": float(np.quantile(a, q)) for q in qs}\n",
        "    out[\"min\"] = float(np.min(a))\n",
        "    out[\"max\"] = float(np.max(a))\n",
        "    out[\"mean\"] = float(np.mean(a))\n",
        "    out[\"std\"] = float(np.std(a))\n",
        "    return pd.Series(out)\n",
        "\n",
        "def sample_skew(a):\n",
        "    a = np.asarray(a, dtype=float)\n",
        "    a = a[np.isfinite(a)]\n",
        "    m = a.mean()\n",
        "    s = a.std()\n",
        "    if s == 0:\n",
        "        return 0.0\n",
        "    return float(np.mean(((a - m) / s) ** 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. 小型演示：右偏数据的变换效果（可用于解释）\n",
        "\n",
        "说明：这里用合成数据演示“右偏 + MSE 尾部主导”的直观现象，并展示 log1p / Yeo-Johnson 的压尾效果。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "rng = np.random.default_rng(0)\n",
        "\n",
        "# 合成右偏特征 X 与右偏目标 y\n",
        "n = 3000\n",
        "X = rng.lognormal(mean=0.0, sigma=1.0, size=n)  # 非负右偏\n",
        "y = 2.0 * X + rng.lognormal(mean=0.0, sigma=1.2, size=n)  # 右偏目标\n",
        "\n",
        "print('X skew:', sample_skew(X))\n",
        "print('y skew:', sample_skew(y))\n",
        "print('\\nX quantiles:\\n', quantile_summary(X))\n",
        "print('\\ny quantiles:\\n', quantile_summary(y))\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.hist(X, bins=60)\n",
        "plt.title('Raw X (right-skew)')\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.hist(y, bins=60)\n",
        "plt.title('Raw y (right-skew)')\n",
        "plt.show()\n",
        "\n",
        "# log1p\n",
        "X_log = np.log1p(X)\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.hist(X_log, bins=60)\n",
        "plt.title('log1p(X)')\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.hist(y_log, bins=60)\n",
        "plt.title('log1p(y)')\n",
        "plt.show()\n",
        "\n",
        "# Yeo-Johnson（允许负，示例中也可用）\n",
        "pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "y_yj = pt.fit_transform(y.reshape(-1,1)).ravel()\n",
        "\n",
        "fig = plt.figure(figsize=(6,4))\n",
        "plt.hist(y_yj, bins=60)\n",
        "plt.title('Yeo-Johnson(y) (standardized)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. 表述\n",
        "\n",
        "“X 的右偏主要是尺度/表示问题，影响距离度量、正则化与数值稳定性；若使用线性/距离敏感模型，常见处理是 log1p、Yeo-Johnson 或稳健缩放。\n",
        "\n",
        "Y 的右偏直接影响损失函数含义，原尺度 MSE/RMSE 会被尾部样本主导，并常伴随异方差与残差偏态；因此在建模前先检查 y 分布，必要时对 y 做 log1p 或 Yeo-Johnson，或改用更稳健的损失函数。训练在变换空间进行，最终用 inverse transform 回原尺度并用原尺度指标复核。”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. 最小落地代码片段（sklearn pipeline 参考）\n",
        "\n",
        "### 8.1 只变换 y：TransformedTargetRegressor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.compose import TransformedTargetRegressor\n",
        "from sklearn.preprocessing import FunctionTransformer, PowerTransformer\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# log1p(y) 版本（y>=0）\n",
        "ttr_log = TransformedTargetRegressor(\n",
        "    regressor=Ridge(alpha=1.0),\n",
        "    func=np.log1p,\n",
        "    inverse_func=np.expm1,\n",
        ")\n",
        "\n",
        "# Yeo-Johnson(y) 版本（y可负）\n",
        "# 注：PowerTransformer 自带 inverse_transform，可直接用\n",
        "yj = PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "ttr_yj = TransformedTargetRegressor(\n",
        "    regressor=Ridge(alpha=1.0),\n",
        "    transformer=yj,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 同时处理 X（可选）\n",
        "- 若使用线性/距离敏感模型：X 做 log1p 或稳健缩放。\n",
        "- 若使用树模型：一般不强依赖 X 变换，但极端值仍可考虑 clip。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 示例：X 做 log1p + 标准化（用于线性模型）\n",
        "X_transform = FunctionTransformer(np.log1p, validate=False)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('x_log1p', X_transform),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('model', Ridge(alpha=1.0)),\n",
        "])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "X_vs_Y_right_skew_notes.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
